<a class="mk-toclify" id="table-of-contents"></a>

# Table of Contents
- [Machine Learning Yearning](#markdown-toc-1)
    - [1. 为什么要研究机器学习策略](#markdown-toc-2)
    - [2 怎么使用这本书来帮助你的团队](#markdown-toc-3)
    - [3 前提条件和记号](#markdown-toc-4)
    - [4 规模驱动机器学习进步](#markdown-toc-5)
- [建立dev和test集](#markdown-toc-6)
    - [5 你的dev和test数据集](#markdown-toc-7)
    - [6 你的dev和test集应该来自同样的分布](#markdown-toc-8)
    - [7 你的dev和test集的规模应该要多大？](#markdown-toc-9)
    - [8 为你的团队建立单一评价指标去优化？](#markdown-toc-10)
    - [9 可优化指标和需满足指标](#markdown-toc-11)
    - [10 加速dev和test集的每次迭代的训练速度](#markdown-toc-12)
    - [11 什么时候改变dev/test集和评价指标?](#markdown-toc-13)
    - [12 构建dev和test集的经验和教训](#markdown-toc-14)
- [基础的误差分析（Basic Error Analysis）](#markdown-toc-15)
    - [13 快速构建你的系统，然后迭代优化](#markdown-toc-16)
    - [14 误差分析：通过审视dev集的样本来评估想法](#markdown-toc-17)
    - [15 在误差分析中，并行地评估多个想法](#markdown-toc-18)
    - [16 过滤dev和test集中label错误的样本](#markdown-toc-19)
    - [17 如果你有一个巨大的dev集，把它切分成2部分，你只关注其中之一（用其中之一做误差分析）](#markdown-toc-20)
    - [18 Eyeball-dev集和Blackbox-dev集应该多大？](#markdown-toc-21)
    - [19 基础误差分析(Basic error analysis)的总结](#markdown-toc-22)
- [偏差和方差 Bias and Variance](#markdown-toc-23)
    - [20 偏差和方差：误差的两个主要来源](#markdown-toc-24)
    - [21 偏差和方差的例子](#markdown-toc-25)
    - [22 和最佳错误率对比](#markdown-toc-26)
    - [23 处理偏差和方差](#markdown-toc-27)
    - [24 偏差和方差的权衡](#markdown-toc-28)
    - [25 减少可避免偏差的技术](#markdown-toc-29)
    - [26 训练集的误差分析](#markdown-toc-30)
    - [27 减少方差的技术](#markdown-toc-31)
- [学习曲线（Learning curves）](#markdown-toc-32)
    - [28 诊断偏差和方差: 学习曲线](#markdown-toc-33)
    - [29 绘制training集的误差曲线](#markdown-toc-34)
    - [30 解读学习曲线：高偏差](#markdown-toc-35)
    - [31 解读学习曲线：其他案例](#markdown-toc-36)
    - [32 绘制学习曲线](#markdown-toc-37)
- [与人的水平表现对比（Comparing to human-level performance）](#markdown-toc-38)
    - [33 为什我们要与人的水平表现对比](#markdown-toc-39)
    - [34 如何定义人的水平表现](#markdown-toc-40)
    - [35 超越人的水平表现](#markdown-toc-41)
- [不同分布的训练和测试](#markdown-toc-42)
    - [36 什么时候你应该在不同分布上训练和测试](#markdown-toc-43)
    - [37 如何决定是否使用所有的数据](#markdown-toc-44)
    - [38 如何决定是否要包含不一致数据](#markdown-toc-45)
    - [39 给数据赋予不同权重](#markdown-toc-46)
    - [40 从training集到dev集的泛化](#markdown-toc-47)
    - [41 识别偏差、方差和数据不匹配导致的误差](#markdown-toc-48)
    - [42 解决数据不匹配问题（data mismatch）](#markdown-toc-49)
    - [43 人工数据合成（artificial data synthesis）](#markdown-toc-50)
- [调试推理算法（Debugging inference algorithms）](#markdown-toc-51)
    - [44 优化验证试验](#markdown-toc-52)
    - [45 优化验证试验的一般形式](#markdown-toc-53)
    - [46 强化学习实例](#markdown-toc-54)
- [端到端的深度学习](#markdown-toc-55)
    - [47 端到端深度学习的兴起](#markdown-toc-56)
    - [48 更多端到端学习的例子](#markdown-toc-57)
    - [49 端到端学习的优点和缺点](#markdown-toc-58)
    - [50 选择pipeline：数据可用性](#markdown-toc-59)
    - [51 选择pipeline：任务简单性](#markdown-toc-60)
    - [52 直接学习富输出（rich outputs）](#markdown-toc-61)
- [对系统的每个组件做错误分析](#markdown-toc-62)
    - [53 对pipeline的每个组件的错误分析](#markdown-toc-63)
    - [54 把错误归因到pipeline的每个组件](#markdown-toc-64)
    - [55 错误归因的一般情形](#markdown-toc-65)
    - [56 对各组件的错误分析和与人的水平表现比较](#markdown-toc-66)
    - [57 发现有缺陷的ML pipeline](#markdown-toc-67)
- [结论](#markdown-toc-68)
    - [58 打造一个超级英雄团队 - 让你的队友阅读本书](#markdown-toc-69)

<div class="mk-toclify" id="markdown-toc-1"></div> 

# Machine Learning Yearning 

![book_front.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/book_front.png)

![book_front_2.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/book_front_2.png)

<br>

----------

> 在这些章节中，您将会学习到，如何在团队统一机器学习的优化策略和方向，也会学习到如何建立dev集和test集。关于如何建立dev/test集的建议已经随着机器学习向着更大的数据集发展而发生了变化，在现代的机器学习项目，你应该如何处理这种情况。

<br>

<div class="mk-toclify" id="markdown-toc-2"></div> 

## 1. 为什么要研究机器学习策略 
机器学习是数以百计重要的应用程序的基础，包括网络搜索引擎、邮件反垃圾、语音识别、产品推荐等等。这里我假设你或者你的团队正在从事机器学习的项目，你目标想获得快速的进展，希望这本书能够帮你完成这样的目标。

**例子：建立一个识别猫图片的项目**

假设你现在正在建立一个项目，目标是为猫的爱好者提供猫图片的实时流展示系统。

![chapter_1_cat](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_1_cat.png)

假设你现在使用神经网络来建立一个计算机视觉识别系统来检测猫的图片。

不过悲剧地是，你的系统的准确度还不够好，给你造成了很大的压力，你会怎么做？
你的团队提出了很多想法，比如：

- 获取更多的数据：收集更多的猫图片
- 丰富训练集的多样性，比如收集在不经常地方出现的猫图片，不常见颜色的猫、用不同照相机拍摄的猫图片
- 延长训练时间，迭代更多梯度下降的步数
- 尝试规模更大的神经网路，比如增加更多层数/隐藏单元节点/不同的训练参数
- 尝试规模更小的神经网路
- 增加正则化（如L1或者L2正则化）
- 改变神经网络的结构（比如激活函数、隐藏节点的个数）
- ...

如果你选择正确了，你会成功地完成这个技术领先的猫图片识别系统，并且带领你的公司走向成功，但如果选择错了，你可能会浪费好几个月的时间。你应该怎么做？

这本书会告诉你怎么做。大多数的机器学习问题留下的经验，会告诉你哪些是有用的尝试，哪些是没用的尝试。通过学习这些经验，能够节省你几个月甚至是几年的时间。

<div class="mk-toclify" id="markdown-toc-3"></div> 

## 2 怎么使用这本书来帮助你的团队 

当阅读完这本书后，你会深入地理解到如何把控一个机器学习项目的技术方向。

不过也许你的团队成员可能不是很理解你的建议和方向。举个例子，也许你让你的团队的定义一个单指标评价指标，不过他们不是很信服。这个时候，你怎么去谁服他们呢？ 

这就是我为什么把这一章内容变得很短: 这样你可以把这章内容打印出来，让你的团队成员能够通过这1-2页纸的内容来获取你让他们知道的东西。

优先级的一些改变会对你的团队的生产力产生巨大的影响。通过帮助你的团队达成一些这样的变化，我希望你能成为你团队的超级英雄！

![chapter_1_superhero](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_1_superhero.png)

<div class="mk-toclify" id="markdown-toc-4"></div> 

## 3 前提条件和记号 

如果你学习过机器学习课程，比如我在coursera的机器学习网络课程，或者你曾经在监督学习上有过经验，你将能够理解这节内容。

我假设你对监督学习比较熟悉：根据打过标签的样本数据，学习一个函数，把x(样本）映射成y（标签）。可监督学习算法包括线性回归，逻辑回归和神经网络。机器学习有非常多的形式，但是机器学习的实际价值大多数都来自于监督学习。

我将经常提到神经网络（也称为深度学习）。往下看这篇文章，你只需要对神经网络有一个基本的理解。

如果你对此不太熟悉，需要先花3周时间看看我在Coursera的机器学习课程。http://ml-class.org
![cat](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_3_coursera.png)

<div class="mk-toclify" id="markdown-toc-5"></div> 

## 4 规模驱动机器学习进步 

深度学习（神经网络）的思想已经存在几十年了。为什么现在才得到广泛发展？

最大的原因有两个：

- **有用数据的规模**。 当今人们花越来越多的时间在数字设备上（比如笔记本、手机）。这些数字设备产生了巨大规模的数据，让我们的深度学习算法有了用武之地。
- **计算规模的扩展**。这几年，我们终于开始有能力去训练一个足够大规模的神经网络，去训练巨大规模的数据。

具体地说，即使你收集到了更多的数据，传统的机器学习算法，如逻辑回归，会遇到效果和性能瓶颈，即所谓的plateaus.这意味着这些算法的学习曲线会出现flattens out,效果无法再提升。

![cat](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_4_plateaus.png)

也就是说，传统的机器学习算法，对这么巨大规模的数据无能力为。

如果你用一个小规模的神经网络（NN）来代替一个传统的机器学习算法，你可能只会获取些微效果的提升。

![cat](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_4_plateaus_2.png)

这里所说的小规模的神经网络，是指只使用少量的神经单元节点、隐藏层、参数）。最后，如果你训练越大的神经网络，你可以获得更好的效果。 神经网络规模越大，效果会越好。

![cat](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_4_plateaus_3.png)

（上图表示，神经网络在小规模的数据集上效果更好。实际上可能不一定。在小规模数据集中，传统的机器学习算法可能会表现得更好，这取决于人工构造的特征得好不好。比如如果你有20个样本，使用逻辑回归或者神经网络都不是那么重要，人工构造的特征比算法本身影响更大。不过如果你有1百万个样本，我会更倾向使用神经网络。）

这样，如果你想获得最好的效果，就训练一个很大的神经网络，这样你就可以获得像上图的绿线，但是必须要有规模足够大的数据。

其他细节也很重要，比如神经网络的结构。神经网络的结构一直以来有很多创新的地方。不过，目前为止，最有效的提高算法的效果的方法之一仍然是训练更大的神经网络和获取更多的数据。

要想训练更大的神经网络和获取更多的数据，往往是出于意外地复杂。本书将会讨论详细的细节。我们先从传统机器学习算法优化的一般策略开始，最后建立一个有最优优化策略的深度学习系统。


<div class="mk-toclify" id="markdown-toc-6"></div> 

# 建立dev和test集

<div class="mk-toclify" id="markdown-toc-7"></div> 

## 5 你的dev和test数据集

让我们回到之前提的猫图片识别的例子：你开发了一个手机app，用户通过你的app上传很多类别的图片。你希望能够自动识别其中的猫的图片。 

你的团队从其他网站下载得到了很多猫的图片(正样本）和不是猫的图片（负样本）。他们按照70%：30%来划分训练集和测试集。在这些训练集和测试集之上，他们训练了一个效果良好的猫图片识别系统。

但是，当你真正把这个系统上线后，你会发现效果差得可怜。

![chapter_5_performance_poor](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_5_performance_poor.png)

发生了什么事情？

你发现，用户通过你app上传的图片和你从其他网站下载的图片往往不一样。用户上传的的图片来自于手机照片，分辨率较低、更加模糊、光线更差。因为你的训练集和测试集来自其他网站，你的算法不能很好地适应这些手机照片。

在大数据时代之前，机器学习领域有一个常用的定律：随机70%和30%来切分数据作为training集和test集。这种做法没问题，不过当应用中的训练集中数据的分布（比如我们例子中的网站图片）和真实应用的数据（如例子中的手机照片）差别很大的时候，这是一个坏主意。

我们通常定义：

- **training集** -- 你们算法真正面对的数据。
- **dev集**  -- 用来调参、选择特征和其他优化算法。有时候也叫做验证集（**hold-out cross validation set**）
- **test集** -- 用来评估算法的效果，不用来调参和和优化算法。

一旦你定义好dev集和test集，你的团队会尝试各种方法，比如各种参数，去看看哪种方法表现更好。这个dev和test集让你的团队能够快速地了解你的算法有多好。

**换句话说，dev和test集的目的是引导你的团队往机器学习系统的最重要的优化方向前进。**

因此，你应该做如下的事情：

从真实的数据中选择dev和test集

换句话说，你的test集不应该是简单的30%，特别是你真实的数据（手机图片）和你的训练集（网站图片）差别很大的时候。

如果你的手机app还没上线，你可能还没有用户，这样可能无法获取到真实的数据。不过你仍然可以尝试接近这些数据，比如让你的朋友把他们手机上猫的照片发给你。一旦app上线，你就可以用真实用户的数据来更新你的dev和test集。

如果你真的无法获得真实的数据，你可以尝试使用网站的图片。不过你应该意识到，这样可能会导致效果并不如想象地好。

在dev和test集的收集上花费多少代价是需要评估判断的。不过不要假设你的training集和test集的数据的分布是一样的。尝试从真实的数据中构造test集，而不是从你目前作为training集的数据。

<div class="mk-toclify" id="markdown-toc-8"></div> 

## 6 你的dev和test集应该来自同样的分布 

你把你的猫手机图片分为四个区域类别，根据如下的最大市场：1. 美国。 2. 中国。 3.印度。 4.其他。我们把美国和印度放到dev集，把中国和其他放到test集。换言之，我们把任意的2个区域类别作为dev集，其他的另外两个作为test集。对吗？


![cat](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_6_wrong.png)

一旦定义好dev集和test集，你的团队就可以集中精力提高dev集的效果。这样，dev集应该反馈你真实的数据：在所有的区域上的图片都表现好，而不是仅仅在四个区域之中的两个。

dev和test集的分布不一样还有第二个问题：有一种可能，你的团队发现在dev集上表现良好，但在test集上却表现很差。我看到过很多这种令人沮丧和浪费努力的情形。不要让这些情形在你的团队中发生。

举个例子，假设你的团队在dev集上表现良好，却在test集上表现很差。如果你的dev和test集都来自同样的分布，这样你就可以有一个很清晰的分析结论：你在dev集上过拟合了。比较明显的做法是去增加你的dev集的数据。

但是，如果如果你的dev和test集都来自不一样的分布。这样你的选择没那么清晰。有几件事可能出了问题。

1. 你在dev集上overfit了。
2. test集比dev集更复杂（harder）。所以你的算法可能已经做得足够好，但是不太可能有进一步的重大提升。
3. 你的test集不仅仅更复杂（harder），而且和dev集分布不一样。这样在dev集表现良好不一定也在test集也表现良好。在这种情况下，你的很多去提高dev集效果的努力往往都是白费的。

在机器学习上应用工作本来就已经很困难了。dev和test集的分布不一致的问题引入了额外的不确定性，你不知道是应该先去改善dev集的分布一致性还是提高在test集的效果。因为你很难区分哪个才是真正的原因，从而会影响你解决问题的优先级。

如果你是在解决一个第三方提供的基准问题，问题的建立者可能会保证dev和test集来自同样的分布。影响你在这些基准测试的效果表现,更大的因素是运气、而不是技巧。研究在一个分布的数据上训练，却能够在不同的另外一个分布的数据上表现良好的机器学习算法，是一个很重要的研究领域。但是如果你的目标是去解决一个实际的机器学习工程问题而不是研究的话，我建议你从同一分布的数据中选择你的dev和test集。这样会让你团队的效率更高。

<div class="mk-toclify" id="markdown-toc-9"></div> 

## 7 你的dev和test集的规模应该要多大？ 

你的dev集应该足够大，以便能比较出不同算法的优劣。举个例子，分类器A的准确率（Accuracy）是90.0%，而分类器B的准确率是90.1%。这样只有100个样本的dev集就不足以检测出0.1%的区别。根据我所见的不同的机器学习问题，只有100样本的dev集是很小的。dev集的大小通常在1000到10000之间。假设是10000样本的dev集，你有很大的概率能够检测出0.1%的效果提升。*（理论上，你也可以测试一个算法在dev集上是否有足够统计学意义上的改进。实际上，大多数团队不会去理会这个（除非他们在发学术论文），我通常不会觉得这种统计意义上的测试有用。）*

对于成熟且重要的产品，比如广告、网页搜索和产品推荐，我看到这些团队非常积极地追求甚至是0.01%的改进。因为它对公司的利润有直接的影响。在这种情况下，dev集的规模可以比10000要大很多，以便能够检测出更小的改进。

test集的大小如何呢？它应该足够大，以便能够高可信地评估算法的整体效果。一种流行的启发式方法是使用30%的数据作为你的test集。当你有100-10000个样本数据的时候，这种方法没问题。但在当今大数据时代，我们的机器学习问题面临的数据往往是超过十亿规模，dev和test集的比例一直在减少，但绝对数量却一直在增加。已经没有必要显性地规定应该使用多大比例的dev和test集来评估算法的效果了。 

<div class="mk-toclify" id="markdown-toc-10"></div> 

## 8 为你的团队建立单一评价指标去优化？ 

分类器的准确率指标是单一评价指标的一个例子。你在你的dev或者test集，跑你的分类器，然后得到一个描述分类正确的比例的数字。根据这个指标，如果一个分类器A的准确率为97%，另外一个分类器B的准确率为90%，我们可以说A要比B好。

相反，正确率（Precision)和召回率(Recall)不是单一评价指标的例子。他们是评价分类器的两个一体化的多评价指标。多评价指标会让算法比较更难。假设你的算法效果如下图所示：
(一个猫图片分类器的正确率是指dev或者test集中，分类器识别为猫图片中正确的比例， 召回率是指，在dev或者test集中真实猫的所有图片中，被分类器正确识别为猫图片的比例。在高正确率和高召回率之间经常存在权衡。)

![classifier_precision_recall](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_8_classifier_precision_recall.png)

这里，没有哪个分类器明显更好，因此，你无法立刻决定选择哪个分类器。

在开发期间，你的团队可能会在算法架构、模型参数、特征选择等方面对模型进行各种尝试。单一评价指标，比如准确率，会让你能够按照这个指标对你尝试的各种模型进行排序，从而快速地选择出更优的那个模型。

如果你真的很重视正确率和召回率，我建议你用一些标准的方法把他们组合成一个单一指标。比如，用他们的平均值作为单一评价指标。或者，你可以用F1值来代替平均值，F1值是另外一种平均值的计算方式，一般情况下比简单的平均值表现地更好。
（如果你想了解F1值，请访问https://en.wikipedia.org/wiki/F1_score，它是正确率和召回率的调和平均数。计算公式为：2/((1/Precision)+(1/Recall))。

![classifier_F1](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_8_classifier_F1.png)

单一评价指标能够加快你选择更优模型的能力。它能够对众多的模型中做出清晰的排序，从而能够清晰地指导整个优化过程的方向。

最后的例子，假设你单独分别评价你的猫图片分类器在下面四个主要市场的准确率：美国、中国、印度和其他。这会有4个指标。通过对这几个指标进行加权平均，你就可以得到一个单一评价指标。平均或者加权评价是最通用的方法能够从多个评价指标中得到一个单一评价指标。

<div class="mk-toclify" id="markdown-toc-11"></div> 

## 9 可优化指标和需满足指标 

这里有其他方法来组合多个评价指标。

假设你对准确率（Accuracy）和算法的运行时间（RunningTime）都很关心。你需要从以下三个分类器选择最优的一个。

![optimizing_classifier](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_9_optimizing_classifier.png)

单纯把准确率和运行时间通过简单的公式组合起来，不是一个自然的方法，比如：
Accuracy - 0.5*RunningTime

你可以这样做：首先，定义一个可接受的运行时间。让我们假设是100ms。然后，在这个运行时间的限制条件下，最大化你的算法的准确率。在这里，运行时间就是所谓的需满足指标（satisficing metric）--你的分类器只需要满足这个评价指标即可。在这个例子，算法最多运行100ms。准确率则是可优化指标。

如果你需要在多个场景指标下做出权衡，比如模型文件的大小（文件大小对手机app很重要，因为手机用户不想下载大文件），运行时间和准确率，你可能需要把N-1个场景指标作为需满足指标（satisficing metric）。你只需要保证这些指标满足一定的条件。然后定义剩余的一个指标为可优化指标。举个例子，把模型文件大小和运行时间分别设定一个阈值，在这两个阈值的条件下，优化模型的准确率。

最后举个例子，假设你现在在开发一个使用麦克风的硬件设备，它听到用户说一个特定的"唤醒词"，就可以让系统醒来。比如Amazon的Echo的唤醒词“Alexa”， Apple的Siri的唤醒词“Hey Siri”，Android的唤醒词“Okey Google”，百度app的唤醒词“Hello Baidu”。假设你对false postive rate（用户没有说唤醒词，系统却被唤醒的比例）和false negative rate（用户说了唤醒词，系统没有被唤醒的比例）这两个指标都关心。一个合理的目标是最大化false negative rate，并且让每24小时的false negative数(用户说了唤醒词，系统没有被唤醒的出现次数)不超过1个。

一旦你的团队对齐了可优化评估指标，他们的项目进展能够进行地更快。

<div class="mk-toclify" id="markdown-toc-12"></div> 

## 10 加速dev和test集的每次迭代的训练速度 

很难事先知道什么方法对新问题有效。即使是一位有经验的机器学习专家在他们取得有用进展之前，通常也需要尝试数十个方法。当构建一个机器学习系统的时候，我通常会这样：

1. 从一些**想法**开始构建这个系统
2. 通过**代码**实现这些想法
3. 通过**实验**去发现这些想法是否有用（一般情况下，我的一开始的想法是没用的！），然后回到最开始，产生更多的其他想法，不断迭代。

![chapter_10_idea_code_experiment](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_10_idea_code_experiment.png)

上图是一个迭代的流程。迭代的时间越短，你构建系统的速度也越快。这就是为什么设定dev和test集和评价指标的重要性原因所在：每次你尝试一个想法，根据这个想法在dev集的评价指标，决定是否继续往下优化。

相反，假设你没有一个dev集和它的评价指标，每次你的团队优化一个新的猫图片分类器，你必须把这个分类器集成进你的app，通过花费几个小时去确定这个新的分类器是否改进了。这种做法难以置信地慢！同时，如果你的团队把分类器的准确率从95.0%提高到95.1%，你可能无法通过app来发现这个0.1%的提高。然而通过逐步积累很多个0.1%的提高，你的系统会变得越来越好。通过建立dev集和它的评价指标，你可以快速地检测出哪些想法让你的系统带来或小或大的改进，从而让你快速决定哪些想法是应该继续优化，哪些想法应该抛弃掉。

<div class="mk-toclify" id="markdown-toc-13"></div> 

## 11 什么时候改变dev/test集和评价指标? 

当开始一个新的项目时，我会尝试快速去选择dev/test集，因为它会让团队有个明确的目标去优化。

我通常要求我的团队在一周之内去设定一个初始的dev/test集和评价指标，极少情况会花费更长时间。从不完美开始行动，而不是过度思考。不过，这个1周期限不太适合成熟的产品应用。比如，反垃圾邮件是一个成熟的深度学习产品应用。我看到过很多团队花费几个月的时间为已经成熟的产品应用得到更好的dev/test集。

如果你在后来发现你一开始的dev/test集或者评价指标漏洞百出，那就马上想尽一切方法去改变它们。举个例子，如果你的dev集和评价指标认为分类器A比B好，但你的团队认为B对你们的产品更优，这也许是一个信号，让你指定新的dev/test集或者评价指标。

有三个可能的主要原因导致dev/test集和评价指标错误地认为分类器A更好：

1. 真实数据的分布和dev/test集的不一样。

假设你一开始的dev/test集主要是成年猫。你上线后，发现用户更多地上传幼年猫，这样，dev/test集的数据分布不能代表真实数据的分布。在这种情况下，更新你的dev/test集是更具代表性的做法。

![chapter_11_child_cat](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_11_child_cat.png)

2. 在dev集上过拟合了（overfit）。

在dev集上不断迭代训练优化会导致你的算法在dev集上逐步地“过拟合”。当你完成训练，你会在test集上评估效果。如果你发现dev集上效果远比test集上要好，这是一个信号，表示你已经在dev集上过拟合了。在这种情况下，应该马上去获取新的dev集。

如果你需要跟踪你团队的优化进度，你可以定期在test集评价你的系统，比如每隔1个星期或者1个月。不过不要通过test集来做出任何关于算法的决定，包括是否回滚到上周的系统。如果你这样做了，你会开始在test集上过拟合，从而不能指望它来对你系统给出完全无偏的效果评估（你可能需要用在发表学术论文、或者可能使用这个评价指标来做一些重要的商业决定）。

3. 这个评价指标评价的不是你的项目所需要优化的。

假设回到刚才的猫图片分类器，你的评价指标是准确率。这个指标认为分类器A比B要优。但是假设你对两个分类器都进行了实验，发现分类器A偶尔会导致色情图片识别通过，即使A的准确率更高，但它偶尔会	误判色情图片的问题导致它的表现是不可接受的。这个时候你应该怎么办？

在这里，准确率这个评价指标，其实不能用来评估分类器B比A要更好这个事实。所以，你不能再通过准确率这个指标来决定哪个算法更优。是时候改变你的评价指标了。比如，你可以让你的评价指标对误判色情图片进行很严重的惩罚。我很强烈地建议重新定义一个新的评价指标，显式地为团队设定新的优化目标，而不是长期停留在因为没有置信度高的评价指标而不得不手动择优分类器的阶段。

在项目的周期中，改变dev/test集或者评价指标是很普遍的行为。 设定一个初始的dev/test和评价指标能够让你快速迭代优化。如果你发现dev/test集或者评价指标已经无法指引你团队的优化方向，这没什么大不了的。改变他们，并确保你的团队知道这个新的方向。

<div class="mk-toclify" id="markdown-toc-14"></div> 

## 12 构建dev和test集的经验和教训 

- 从和真实数据同一分布的数据中选择dev和test集。它们的分布可能和你的训练集不一样。
- 尽可能让你的dev和test集的数据分布保持一致。
- 选择一个单一评价指标来作为团队优化目标。如果有多个指标，考虑把他们合并成一个指标（比如求和评价）或者定义一个需满足指标和可优化指标。
- 机器学习是一个快速迭代的过程。在你发现满意的进展的时候，可能你需要尝试数十次。
- 拥有dev/test集和单一评价指标能够帮助你快速评估你的算法和迭代地更快。
- 当你开始一个新的项目的时候，尽快地建立一个dev/test和评价指标，通常在一周之内。对于成熟的产品可能花费更长时间。
- 以往70%:30%切分训练集和dev/test集的启发式做法，不适合你有大量数据的情况。dev/test集的大小可以远远少于30%的数据。
- 你的dev集的大小应该足够大到能够检测出你算法的有意义的改进，不过也没必要太过大。你的test集应该足够大，大到能够对你的系统的效果给出高可信度的评估。
- 如果你的dev集和评价指标已经不再能够指导你团队的优化方向了，那么快速地改变他们：(i) 如果在dev集上过拟合了，去获取更多的dev数据。（ii) 如果真实数据的分布和dev/test集的分布已经不一样了，获取新的dev/test集数据。（iii）如果你的评价指标已经不再衡量和代表你所关心的目标，请重新定义评价指标。

<div class="mk-toclify" id="markdown-toc-15"></div> 

# 基础的误差分析（Basic Error Analysis） 

<div class="mk-toclify" id="markdown-toc-16"></div> 

## 13 快速构建你的系统，然后迭代优化 

假设你想构建一个反垃圾邮件系统，你的团队有如下的几个想法：

- 收集大量的垃圾邮件作为训练集。比如，建立一个“蜜罐”，故意地向已知的垃圾邮件发送者发送假冒的电子邮件地址，这样你就可以自动收获他们发送到这些地址的垃圾邮件。
- 对邮件的文本进行语义分析，抽取特征。
- 对邮件的header进行分析，抽取相关特征来挖掘邮件经过的互联网服务器的集合。
- 等等。

即使我曾经在反垃圾邮件系统全面地工作过，我现在还是很难决定先选择哪个想法。如果你不是在这个领域的专家，那就更难了。

所以不要尝试一开始就设计和构建一个完美的系统。相反，快速构建和训练一个基本的系统--可能在几天之内*（这个建议只适合那些希望构建AI应用的读者，而不是那些希望发学术论文的研究学者。后续我会回到做学术研究的话题）*。即使这个基本的系统离完美的系统还远得很，它也已经有足够的价值来让你检查基础系统的功能：你可以很快找到线索，引导你把你的时间投入到最有希望的优化方向。下面这几章会告诉你怎么找到和理解这些线索。

![chapter_13_read_clue.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_13_read_clue.png)

<div class="mk-toclify" id="markdown-toc-17"></div> 

## 14 误差分析：通过审视dev集的样本来评估想法 

![chapter_14_evaluate_ideas.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_14_evaluate_ideas.png)

当你在开发你的猫图片识别系统的时候，你注意到了被错误识别为猫图片的狗图片样本。某些狗真的是很像猫!

一个团队成员建议和一个第三方软件合作，会让系统更好地处理狗的图片。这个工作可能会花费1个月的时间。这个团队成员对这个想法非常兴奋。你会要求他们这样前行么？

在决定花费1个月时间在这个工作之前，我建议你先预估一下这个工作实际带来的对准确率的收益。这样你就可以理性地决定是否值得花费这一个月的时间，或者把这个月的时间安排在其他任务上。

具体地说，你可以做如下的事情：

1. 从dev集中误分类的图片样本中选取出100个图片样本。
2. 人工分析这些图片样本，计算他们是狗图片的比例。

这种人工分析误分类样本的过程，称之为**误差分析（error analysis）**。在这个例子里，如果你发现错误分类的图片中有5%是狗，这样无论你尽多大努力优化算法对狗图片的识别，你最多只会降低5%的错误率。换句话说，5%是这个合作给你的系统带来的收益的天花板。因此，如果你的系统的准确率是90%（10%错误率），这个合作最多能将准确率提高到90.5%（或者说9.5%的错误率，比之前的10%错误率降低了5%）。

相反，如果你发现错误分类的图片中，有50%都是狗图片。这样你更自信地判断出这个建议的合作对你的项目有巨大的影响。它能把准确率从90%提高到95%（错误率降低了50%，从10%降到5%）。

这个对错误分析的简单的统计能够让你迅速地估计出与第三方软件合作的价值。它对决定这个合作是否值得去做，提供了一个定量的分析依据。

误差分析，通常可以帮助你找出不同方向有多少前途价值。我看到过很多工程师不愿意花费时间在误差分析。他们往往更热衷于着手实现一个想法，而不是判断这个想法是否值得花费时间去实现。这是一个常见的错误：这可能会导致你的团队花费了一个月的时间，最后却发现收益微乎其微。

人工分析100个样本不会花费很长时间。即使你1分钟分析一个样本，你也会在2个小时之内完成。这2个小时的时间能帮你避免浪费几个月的时间。

**误差分析（error analysis）**是指对dev集中错误分类的样本进行分析的过程，通过这个过程，你可以理解这些错误分类的深层原因。它能帮助你更好对你的项目进行优先级的规划-就像这个例子-同时也能启发新的方向，我们将在下面讨论。接下来的几章还将介绍进行误差分析的最佳实践。

<br>

----------

> 当你开始一个新的机器学习项目的时候，你如何找到最有价值的优化方向？这些章节描述了人工误差分析过程的机制，这将帮助你为你的项目找到最有价值的优化方向。
>   
> 请阅读这周的文章，学习更多（第15-19章）!

<br>

<div class="mk-toclify" id="markdown-toc-18"></div> 

## 15 在误差分析中，并行地评估多个想法 

你的团队现在有几个关于优化猫图片识别系统的想法：

- 修正你的算法把狗图片识别为猫图片的问题。
- 修正你的算法把猫科动物（狮子、豹等）识别为家猫（宠物）的问题。
- 提高对模糊图片的识别效果。
- 。。。

你可以高效并行地评估所有的想法。我通常会创建一个表格，把100个错误识别的图片样本的分析结果放入到里面。同时我也会记下一些帮助我记住某些具体图片样本的评论。为了说明这个过程，让我们来看看这个表格，你可以用一个小的dev集来生成这4个图片样本。

![chapter_15_spreadsheet.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_15_spreadsheet.png)

第3个图片样本既是猫科动物又模糊。进一步说，因为有可能一个图片样本被分到多个类别，最后一行的百分比之和可能不等于100%。

虽然你是一开始先制定某些错误分类(Dog, Great cat, Blurry），再手动地把一个个样本归类，但是在你分析的过程中，你可能会发现新的错误分类。举个例子，比如说你通过几十张图片发现了Instagram过滤后的图片有很多错误。你可能回过头，在表格增加Instagram一列。人工对错误分类的图片样本进行分析，并以人的角度去看能否对这些图片正确标记，这个方法经常能激励你想出新的错误分类和解决办法。

最有用的错误分类是哪些你有解决方法的分类。比如，那个Instagram错误分类是最有用的，因为你只要撤销Instagram过滤的操作，恢复到原始的图片即可。但是你也不必把自己局限在那些有解决方法的错误分类。这个过程的目标是为了让你建立一个直觉，这个直觉能够帮助你判断哪些优化方向最有价值。

误差分析是不断迭代的过程。不过担心一开始的时候想不出什么错误分类。当你分析完一些图片之后，你可能就会有一些关于错误分类的想法。当你人工把一些图片归类之后，你可能会想到新的错误类别，并且重新对图片进行新错误类别的分类，然后不断重复这个过程。

假设你已经完成了对dev集的100个错误分类的图片样本进行误差分析了，最后你得到如下的表格：

![chapter_15_spreadsheet_2.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_15_spreadsheet_2.png)

从这个表格，你可以知道，解决狗图片的错误类别最多能够减少8%的错误，解决猫科动物或者模糊图片这两个错误类别，能够减少最多的错误。因此，你可能优先选择去解决这两个错误类别的其中一个。如果你的团队人力充足的话，你也可以让你的工程师同时解决这两个错误类别。

误差分析不会产生一个严格的数学公式告诉你哪些是优先级最高的任务。它会促使你必须综合考虑到解决不同的错误类别带来的多大的收益以及工作量。

<div class="mk-toclify" id="markdown-toc-19"></div> 

## 16 过滤dev和test集中label错误的样本 

在误差分析的过程中，你可能会发现你的dev集的某些样本的label是错误的。这里说的“label错误”，是指在算法处理它之前，这些图片被人们错误的标成了错误的label，比如，在样本(x，y)，有一个不正确的label y。举个例子，可能有一些没有猫的图片被错误标记成猫图片。如果你担心label错误的样本比例过高，增加一个错误类别来跟踪这种情况：

![chapter_16_spreadsheet_mislabeled.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_16_spreadsheet_mislabeled.png)

你应该修正dev集样本的label么？请记住，dev集的目标是帮你快速评估算法，这样你可以很快区分出算法A和B哪个更好。如果dev集样本label错误的比例阻碍你做出这样的判断，那么就值得花时间来修正dev集的错误label。

比如，假设你的分类器的效果如下：

- dev集的整体准确率..................90%（10% 错误率）
- dev集因为label错误导致的错误率......0.6%（dev集10%错误率中的6%）
- dev集因为其他原因导致的错误率........9.4%（dev集10%错误率的94%）

这里，相比9.4%，因为label错误导致0.6%的不准确率可能并不足够严重到你需要腾出精力去优先解决。修复dev集中label错误的样本不会有害，但这样做并不重要。你的系统的错误率是10%还是9.4%，可能是一件无伤大雅的事情。

假设你持续优化你的猫图片分类器，取得了如下的效果：

- dev集的整体准确率...................98%（2.0% 错误率）
- dev集因为label错误导致的错误率.......0.6%（dev集2.0%错误率中的30%）
- dev集因为其他原因导致的错误率........1.4%（dev集2.0%错误率的70%）

30%的错误率是因为label错误，这对准确率的评估产生了显著的错误影响。现在就值得去提高dev集中label的质量。解决label错误的问题能帮助你确定你的分类器的错误率是更接近1.4%还是2%-这个差异还是相当显著的。

一开始就容忍dev/test集中样本label错误并不常见（意思是一般情况下一开始不会容忍dev/test集出现label错误的样本）。但随着你的系统不断改进，你会改变主意，因为相对于不断降低的总的错误率，label错误的样本比例会不断增长。

最后的一章会阐述怎么通过算法的优化来降低错误类别的比例（比如狗类别、猫科动物类别和模糊类别），在本章中，你学到了可以通过修正样本的label来处理label错误类别的问题。

不管你用什么方法修正dev集的label，记住一定要将其应用到test集的label，这样你的dev和test集的样本数据才能继续保持服从相同的分布。一起修正你的dev和test集可以避免出现第六章我们讨论的问题，避免你的团队一直是在dev集上做算法的优化，但后来才意识到它们是基于一个不同的test集以不同的标准进行判断算法的优劣的。

如果你决定去提高label的质量，考虑用double-check的方法，对误分类的样本以及正确分类的样本都进行检查。因为可能原始的label和你学习到的算法都是错误的。如果你仅仅只是修正误分类样本的标签，可能会导致你的评价指标是有偏的。如果你的dev集有1000个样本，并且你的分类器的准确率为98.0%，很容易算出，错误分类的样本个数是20个，正确分类的样本数是980个。因为实际上对错误分类的样本进行分析更加容易，会导致你的dev集慢慢地出现有偏的情况。如果你目标是为了开发一个产品或者应用，这个有偏是可以接受的。但是如果你是打算在学术论文或者某些需要test集上的无偏评价指标的情况中使用，这个有偏会 成为一个问题。

<div class="mk-toclify" id="markdown-toc-20"></div> 

## 17 如果你有一个巨大的dev集，把它切分成2部分，你只关注其中之一（用其中之一做误差分析） 

假设你有一个包含大概5000个样本的巨大的dev集，错误率有20%。这种情况下，你的算法会错误分类大概1000个图片样本。分析完这1000个图片样本需要花费很长的时间，所以我们可能需要决定在误差分析过程中不使用所有的图片样本。

在这个例子中，我会显式地把dev集切分为2部分，你只关注其中之一。你可以更快速对过拟合你所关注的那部分。你可以使用你不关注的那部分来优化模型和算法的参数。

![chapter_17_look_at.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_17_look_at.png)

让我们继续上述的例子，dev集中的5000个样本中，你的算法把其中的1000个样本错误地分类了。假设我们希望对其中的100样本进行误差分析（错误分类中的10%）。你应该从dev集随机挑选10%的样本，我们不妨把这些样本叫做Eyeball-dev集，目的是为了提醒我们是用自己的眼睛来对它们做误差分析。（对于一个语音识别项目，你需要对音频进行收听，也许你会把这个集合称为Ear-dev集）。因此这个Eyeball-dev集大概有500个样本，我们希望我们的算法错误分类的数量只有100。

dev集的第二部分，我们称之为Blackbox-dev集，这个集合大概有4500个样本。你可以使用Blackbox-dev集来自动评估分类器的错误率。你也可以使用它来对各种算法进行择优或者调优超参数。	然而，你应该避免去关注分析这个集合的样本。我们称之为“Blackbox”，是因为我们只用这个集合的样本去得到对分类器的“黑箱”评价指标。

![chapter_17_blackbox.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_17_blackbox.png)

为什么我们显式地把dev集切分为Eyeball和Blackbox两部分呢？因为你（通过对Eyeball-dev集的样本分析和优化）会得到关于Eyeball-dev集的样本的直觉经验知识，你就会更快地对Eyeball-dev集产生过拟合。如果单纯看评价指标，你会发现在Eyeball-dev集的指标提高的速度远比在Blackbox-dev集上的快，你已经在Eyeball-dev集产生过拟合了。在这种情况下，你可能需要丢弃原来的Eyeball-dev集，通过从Blackbox-dev集移动更多的数据到老的Eyeball-dev集，或者获取新的样本，来重新生成一个新的Eyeball-dev集。

显式地把dev集切分为Eyeball和Blackbox两部分,可以让你知道你的对错误样本进行手动分析优化的过程，是否会导致你在Eyeball-dev集上产生过拟合。

<div class="mk-toclify" id="markdown-toc-21"></div> 

## 18 Eyeball-dev集和Blackbox-dev集应该多大？ 

![chapter_18_eye.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_18_eye.png)

你的Eyeball-dev集应该足够大，以便于能够通过统计发现你的算法最大的错误类别。如果你正在进行的是一个人类擅长的任务（比如识别猫图片），这里有一些大体的准则：

- 一个有10个错误的Eyeball-dev集被认为是一个很小的集合。只有10个错误的情况下，很难去评估不同错误类别的影响。不过在你只有很少的样本数据，不能再增加Eyeball-dev集的大小的情况下，也好过没有，它对项目的优先级处理也是有帮助的。
- 如果Eyeball-dev集包含大概20个错误，你可以开始粗略地了解主要的错误来源了。（意思是20个错误足够大了）
- 如果是50个错误，你可以很好地了解主要的错误来源了。
- 如果是100个错误，你可以非常好地了解主要的错误来源了。我曾经看到过有人手动分析更多的错误-有时候可达500个之多。只要你有足够的数据，这没有什么坏处。

假设你的分类器的错误率是5%，为了保证你的Eyeball-dev集有大概100个错误，Eyeball-dev集的大小应该在2000左右（因为0.05*2000=100）。为了得到足够的错误样本去分析，分类器的错误率越低，Eyeball-dev集必须越大。

如果你正在进行的是一个人类不擅长的任务，对Eyeball-dev集进行人工分析，可能没什么用处，因为很难分析出算法不能正确分类的原因。在这种情况下，你可能只能忽略掉Eyeball-dev集。我们在后续的章节会讨论怎么来处理这些问题。

![chapter_18_blackbox_dev.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_18_blackbox_dev.png)

那么Blackbox-dev集的大小呢？我们刚才说dev集的大小通常是在1000-10000之间。准确地说，大小在1000-10000之间的Blackbox-dev集通常已经能够提供足够的数据来对超参数进行调优和对多个算法模型进行择优，尽管数据越多，也没什么坏处。一个大小只有100的Blackbox-dev集是很小但仍然是有用的。

如果你只有一个很小的Blackbox-dev集，你可能没有足够的数据切分为足够大的Eyeball-dev集和Blackbox-dev集。反而，你dev集的所有样本可能都被用做Eyeball-dev集--这意味着得对dev集的所有样本进行分析了。

在Eyeball-dev集和Blackbox-dev集之间，我认为Eyeball-dev集更为重要（假设你正在进行的是一个人类擅长的任务，并且人工分析样本明显有收益）。如果你只有一个Eyeball-dev集，在它之上，你都可以进行误差分析、模型选择、超参数调优。只有一个Eyeball-dev集的缺点是在dev集上过拟合的风险会变大。

如果你可以接触到足够多的样本数据，那么Eyeball-dev集的大小主要取决于你有多少时间来分析样本数据。比如，我很少看到有人手动分析1000个以上的错误。

<div class="mk-toclify" id="markdown-toc-22"></div> 

## 19 基础误差分析(Basic error analysis)的总结 

- 如果你开始一个全新的项目，特别在一个你不是很擅长的领域，选择出最正确和最有价值的优化方向是很难的事情。
- 所以不用尝试一开始就设计和构建一个完美的系统。而是尽可能快地创建和训练出一个简单可用的系统-可能在几天之内。然后通过错误分析来帮助你选择出最正确和最优价值的的优化方向，然后如此不断迭代优化你的算法。
- 通过分析dev集中大概100个误分类的样本来执行错误分析，统计出最主要的错误分类。根据这个信息，来决定哪些错误分类优先解决。
- 考虑把dev集切分为Eyeball-dev集（你可以对其中的样本进行人工错误分析）和Blackbox-dev集（你对其中的样本不进行人工错误分析）。如果在Eyeball-dev集的指标比Blackbox-dev集的好很多的话，意味着你在Eyeball-dev集已经过拟合了，应该考虑为Eyeball-dev集收集更多的样本数据。
- Eyeball-dev集的大小应该足够大，大到你的算法错误分类的样本数据足够多，以供你用来分析。对于大多数的应用来说，一个大小在1000-10000个样本的Blackbox-dev集足够大了。
- 如果你的dev集不够大，把它当做Eyeball-dev集，进行人工错误分析（error analysis）、模型选择和超参数调优。

<br>

----------

> 这周的章节，会教你如何通过偏差和方差分析，来优化你的ML项目的优先级排期。关于偏差/方差的旧观念，例如“偏差/方差的权衡”，在深度学习时代变得不那么有用了，现代的机器学习时代的准则需要更新。
>   
> 请阅读这周的文章，学习更多（第20-22章）!

<br>

<div class="mk-toclify" id="markdown-toc-23"></div> 

# 偏差和方差 Bias and Variance

<div class="mk-toclify" id="markdown-toc-24"></div> 

## 20 偏差和方差：误差的两个主要来源 

假设你的training、dev和test集都来自同一分布。然后你应该总是尝试获取更多的训练数据，因为它肯定可以提高效果，对不对？

尽管拥有更多的数据是不会有坏处的，但是不幸的是，它并不总是像你希望的那样有帮助。努力获取更多的数据可能是在浪费时间。那么，你如何决定什么时候应该增加更多的数据，什么时候不增加呢？

在机器学习领域中，误差来自于两个主要的来源：偏差和方差。理解它们有助于你决定是否增加数据，以及其他提高效果的策略，可以更有效率地利用时间。

假设你希望构建一个错误率为5%的猫图片分类器。目前，你的training集的错误率为15%，dev集的错误率为16%。这种情况下，增加更多的训练数据不会有很大的帮助。你应该关注其他的变化。实际上，增加更多的训练样本数据只会让你的算法在训练集上更难有好的效果。（我们在后续的章节会解释原因。）

如果你的training集的错误率是15%（即准确率为85%），但是你的目标是5%（即95%的准确率），第一个要解决的问题是提高你的算法在training集的效果。你的dev/test集的准确率一般来说比在training集上的要更低。所以如果你在已知的training集的准确率只有85%，那么没有任何办法让你的算法在未知数据集上的准确率达到95%。

假设如上述，你的算法在dev集上只有16%的错误率（84%的准确率）。我们把这16%的错误率分成2个部分：

1. 首先，这个算法在training集上的错误率。在这个例子里，是15%。我们可以非正式地把它认为是算法的**偏差**。
2. 其次，这个算法在dev集（或者test集）上比training集上表现地更差了多少？在这个例子里，是差了1%。我们可以非正式地把它认为是算法的**方差**。

(统计学领域上对偏差和方差有更正式的定义，我们不用太在意。粗略地说，当你有一个很大的training集的时候，你的算法的偏差就是它在training集上的错误率。方差就是在test集上比在training集上变差了多少。当你的误差指标是均方误差的时候，你可以写下对这两个量的公式，并且证明总误差=偏差+方差。但是，我们目标是为了讨论如何推进机器学习问题的解决，这里只给出偏差和方差的非正式定义就足够了。)

机器学习算法的一些改进和优化有助于解决第一部分的错误-偏差-从而提高了在training集的效果。有些改进和优化有助于解决第二部分-方差-从而提高了算法从training集到dev/test集的泛化能力。（通过对系统架构做出重大的改变，有一些方法能同时降低偏差和方差。但这些往往难以识别和实施。），为了选择最有用的更改和优化，理解这两个错误部分中哪一部分是当前更迫切需要解决的是非常有价值的。

提高对偏差和方差的直觉理解，将帮助你为你的算法找出有效的改进和优化方向。

<div class="mk-toclify" id="markdown-toc-25"></div> 

## 21 偏差和方差的例子 

回到你的猫分类器的项目，一个“完美”的分类器（比如说人）可能会在这个项目中取得近似完美的结果。

假设你的算法的效果如下：

- training集的错误率 = 1%
- dev集的错误率 = 11%

问题是什么？通过上述章节的定义，我们估算偏差是1%，方差是10%（=11%-1%）。因此，它是**高方差**。这个分类器有着很低的training错误率，却不能泛化到dev集。这也被称作**“过拟合”**。

现在考虑一下这个：

- training集的错误率 = 15%
- dev集的错误率 = 16%

我们估算偏差是15%，方差是1%。这个分类器不能很好地拟合training集，高达15%的错误率。不过，这个错误率在dev集仅仅只比training集高一点点。因此，这个分类器虽然是**高偏差**，但也是“低方差”。我们认为这个算法是**“欠拟合”**。

再考虑一下这个：

- training集的错误率 = 15%
- dev集的错误率 = 30%

我们评估偏差是15%，方差是15%。这个分类器同时有着**“高偏差”和“高方差”**。它在training集上表现很差，因此具有高偏差，并且它在dev集的表现更差，因此也具有高方差。很难说这是过拟合还是欠拟合，因为这个分类器同时既是过拟合，又是欠拟合。

最后，考虑一下这个：

- training集的错误率 = 0.5%
- dev集的错误率 = 1%

这个分类器很棒，是低偏差，又是低方差。恭喜，取得了很棒的效果。棒棒哒。

<div class="mk-toclify" id="markdown-toc-26"></div> 

## 22 和最佳错误率对比 

在我们猫图片识别的例子中，最佳的错误率-也就是最佳的分类器-是接近0%。一个人看一幅图片总是能辨认出它是否有猫的图片。因此，我们希望机器也可以做到。

有其他更难的问题。举个例子，假设你在构建一个语音识别系统，发现有14%的音频有很严重的背景噪声，以至于一个普通人也无法识别它在说什么。在这种情况下，即使是“最佳”的语音识别系统的错误率可能最多只能在14%左右。

假设在这个语音识别系统，你的算法的效果如下：

- training集的错误率 = 15%
- dev集的错误率 = 30%

training集上的错误率已经很接近最佳的错误率14%了。因此，已经没有多大的空间去降低偏差或者提高在training集的效果了。然而，这个算法在dev集的泛化能力比较差，因此，有很大的改善空间去降低方差。

这个例子和上一章的第三个例子比较类似，那个例子中，training集和dev集的错误率同样是15%和30%。如果最佳的错误率是大概0%，那么15%的错误率有很大的改善空间。这表明，降低偏差可能是非常富有成效的。不过如果最佳的错误率是14%的话，这表明这个分类器的偏差已经没有多大的优化空间了。

对于那些最佳错误率远大于0的问题，这里有一些对错误率更详细的细化和分解。继续我们的语音识别例子，dev集30%的整体错误率，可以分解如下（类似的分析可以应用在test集上）：

- **最佳错误率（“不可避免的偏差(unavoidable bias)”）**：14%。假设我们决定，即使是世界上最好的语音识别系统，仍然有14%的错误率。我们可以认为这是算法偏差中不可避免的那一部分。
- **可避免的偏差**：1%。即training集的错误率与最佳错误率的差值。*(如果这个值是负数，表示你的算法在training集上表现得更好。这意味着你在training集上过拟合了，你的算法过于记忆了training集的数据。你应该集中精力去减少方差，而不是减少偏差。）*
- **方差**：15%。即dev集的错误率与training集上的差值

为了把这与我们先前的定义联系起来，偏差和可避免的偏差有如下的相关性：*（这些定义是用来传达关于如何改进机器学习算法的见解的，这些定义与统计学上定义的偏差和方差不一样。从技术上讲，我在这里定义的偏差应该被叫做“偏差带来的错误率（Error we attribute to bias）”，方差应该被叫做“高于最佳错误率的偏差带来的错误率（error we attribute to the learning algorithm’s bias that is over the optimal error rate）”）。*

偏差 = 最佳错误率（“不可避免的偏差”） + 可避免的偏差

这个“可避免的偏差”，反映地是你的算法在training集上和“最佳分类器”的差距。

方差的概念与之前一样。理论上，通过大规模的training集，我们总能把方差减少到接近0。因此，在足够大的样本数据集的前提下，方差都是“可避免”的，因此没有所谓的“不可避免方差”。

在多考虑一个例子，最佳的错误率是14%，并且我们指标如下：

- training集的错误率 = 15%
- dev集的错误率 = 16%

在之前的章节，我们把它叫做高偏差分类器。现在，我们可以说可避免偏差是1%，方差是大概1%。因此，这个算法已经表现得挺好，很难有改善的空间了，和最佳错误率的差距只有2%。

从这些例子中可以看出，知道最佳错误率有助于指导我们的下一步。在统计学，最佳错误率也被称为**贝叶斯错误率（Bayes error rate）**，或者叫贝叶斯率（Bayes rate）。

我们怎么才能知道最佳错误率是多少呢？对于那些人类擅长的任务，比如图片识别或者语音文本转换，你可以让人们标记你training集的样本label，然后计算出这些样本label的准确率。你会得到一个最佳错误率的估计。对于那些人类不擅长的任务（比如预测哪些电影应该被推荐，或者应该展示哪些广告），是很难获得最佳错误率的估计的。

在第33-35章（与人的水平表现（Human-Level）对比），我会详细地讨论比较机器学习算法表现与人的水平表现的整个过程。

在这几章，你学习到了如何通过training集和dev集的错误率，去估算可避免偏差/不可避免偏差。下一章将讨论如何利用这些认识和方法，去决定bias-reduce技术和variance-reduce技术的优先级。根据你项目当前的问题是高(可避免）偏差还是高方差，你应该采用不同的技术。请继续阅读。

<br>

----------

> 一旦你确认了你的算法的问题是高偏差还是高方差，这些章节会讨论解决高偏差和高方差的一些技术。
>   
> 请阅读这周的文章，学习更多（第23-27章）!

<br>

<div class="mk-toclify" id="markdown-toc-27"></div> 

## 23 处理偏差和方差 

这里有处理偏差和方差问题的一些最简单的准则：

- 如果你的问题是可避免的高偏差，增加你模型的规模（比如，增加神经网络的层数和神经元数量）
- 如果你的问题是高方差，扩充你的training集，增加数据量。

如果你能无限扩大神经网络的规模，并且无限扩充你的training集，那么你将能很好地解决很多机器学习问题。

实际上，	增加你模型的规模，最终会导致你遇到计算瓶颈的问题，因为模型规模越大训练速度越慢。你也有可能倾尽全力也无法收集到更多的数据。（即使是在互联网上，猫图片的数量也是有限的）。

不同的模型结构-比如，神经网络不同的结构-会存在不同程度的偏差/方差问题。最近许多深度学习研究已经开发出许多创新的模型结构。所以如果你使用的是神经网络模型，众多的学术文献可以成为你灵感的源泉。在github上同样有很多开源的实现。不过尝试一个新的模型结构的结果，比起简单地扩大模型规模和增加数据量，更加地不可预测。

增加模型的规模通常会减少偏差，但它可能会增加方差，从而可能会导致过拟合。但是，这种过拟合的问题通常只有在你没有用正则化的时候才会出现。如果你有一个设计合理的正则化函数，那么通常你可以安全的扩大模型的规模，而不用担心引起过拟合。

假设你使用的是深度学习，使用L2正则化或者dropout，在dev集上取得很好的效果。如果你增加模型的规模，一般效果会不变或者提高；不太可能会变差很多。避免使用更大模型的唯一原因是增加了计算成本。

<div class="mk-toclify" id="markdown-toc-28"></div> 

## 24 偏差和方差的权衡 

你可能已经听过“偏差和方差的权衡”了。大多数机器学习算法的修改，有些减少偏差，但同时会增加方差。反之亦然。因此，这就造成了偏差和方差之间的“权衡”。

比如，增加模型的规模-在神经网络中增加神经元节点个数和层数，或者增加输入特征-一般会减少偏差，但会增加方差。另外，增加正则化会增加偏差，但会减少方差。

在如今时代，我们通常可以获取到海量的数据和使用规模很大的神经网络（深度学习）。因此在如今时代，会少一些偏差和方差之间的权衡，我们有很多选择可以减少偏差而不损害到方差，反之亦然。

比如，你通常可以通过增加神经网络的规模和优化正则化函数，减少偏差而不显著地增加方差。通过增加训练数据，你也可以减少方差而不影响偏差。

如果你选择了一个合适你的项目的模型结构，你可能会同时减少偏差和方差。选择出这样的模型结构，是一件困难的事情。

在下面几章文章，我们会讨论处理偏差和方差问题的额外的具体技术。

<div class="mk-toclify" id="markdown-toc-29"></div> 

## 25 减少可避免偏差的技术 

如果你的算法遇到了高可避免偏差的问题，你可能可以尝试以下的技术：

- **增加模型的规模**（如神经元节点和层的数量）：这个技术可以减少偏差，因为它可以让你的算法在训练集上拟合得更好。如果它导致方差增加了，那么使用正则化，这通常能消除方差的增加。
- **基于误差分析，修改输入的特征**： 误差分析能够启发你新增新的特征，来帮助你的算法消除特定的错误类别。（我们在后续章节讨论这个)。这些新的特征有助于减少偏差和方差。理论上，增加更多的特征会增加方差；如果你发现了这种情况，那么请使用正则化，这通常能消除方差的增加。
- **减少或者消除正则化**（L2正则，L1正则，dropout）：这会减少可避免偏差，不过会增加方差。
- **修改模型结构**（比如神经网络的结构）：这会让你的模型更适合你的问题：这能同时减少偏差和方差。

还有一种可能没有帮助的方法：

- **增加更多的training数据**：这个方法有助于减少方差，但一般情况下不会对偏差有显著的影响。

<div class="mk-toclify" id="markdown-toc-30"></div> 

## 26 训练集的误差分析 

你的算法必须在训练集上取得很好的效果之后，你才能期待它在dev/test集也有同样的好效果。

除了上述描述的技术可以处理高偏差以外，我有时候也会在training集上应用误差分析的技术，整个过程类似于在Eyeball-dev集上的误差分析。这种方法会有助于解决高偏差的问题-假如它对training集上拟合的不是很好。

比如，假设你现在在构建一个语音系统的系统，已经从志愿者那边收集了一个音频数据的training集。如果你的系统在training集上拟合不是很好，你可能会考虑从算法表现不好的样本中选取一组大概100个样本来听，以此去发现training集中导致错误的主要类别。类似于在dev集上的误差分析，你可以统计不同错误类别出现的数目。

![chapter_26_error_analysis_training_set.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_26_error_analysis_training_set.png)

在这个例子里，你可能已经意识到了你的算法效果不好，首先是因为training集的很多样本都有比较严重的背景噪声。因此，你可能得想办法更好地拟合高背景噪声的样本。

你也可能会找人看看能否对那些高背景噪声的样本进行人工转译。如果因为背景噪声过大，导致人也无法识别出来音频说的内容，我们同样很难期望机器能正确地识别出来。我们将在后面的章节中讨论将算法的水平表现与人的水平表现（human-level performance）进行比较的好处。

<div class="mk-toclify" id="markdown-toc-31"></div> 

## 27 减少方差的技术 

如果你的算法遇到了高方差的问题，你可能会尝试如下的技术：

- **增加更多的training样本**：这是解决高方差的问题的最简单和最可靠的方法，只要你能获得更多的数据和足够的计算能力来处理数据。
- **增加正则化**（L2正则化，L1正则化，dropout）：这个技术可以减少方差，但也会增加偏差。
- **增加early stopping**（根据在dev集的误差指标，提前停止梯度下降）：这个技术可以减少方差，但也会增加偏差。Early stopping表现地和正则化技术很像，有一些作者也会把它叫做正则化技术。
- **利用特征选择来减少输入特征的数量和类型**：这个技术可能会解决方差问题，不过它可能会增加偏差。稍微减少特征的数量（比如从1000个减少到900个）不太可能会对偏差产生巨大的影响。但是如果减少的很多（比如从1000个特征减少大100个，减少了10倍），就可能会对偏差有显著的影响。在现代的深度学习，当样本数据很丰富的时候，特征选择的作用有了明显的降低，因为我们可以把我们拥有的所有特征都放入到算法中，让算法根据样本数据来决定哪些特征会用到，用到的特征的顺序是怎样的。但是如果你的training集规模比较小的话，特征选择还是非常有用的。
- **减少模型的规模**（比如减少神经元节点的数量和层数）：请谨慎使用。这个技术可以减少方差，但可能会增加偏差。然而，我不建议用这个技术来解决方差问题。增加正则化通常会取得更好的效果。减少模型的规模的好处是可以减少你的计算成本，从而可以缩短你模型的训练时间。如果想缩短模型训练的时间，那么想尽一切方法减少模型的规模。不过如果你的目标是减少方差，并且你不关心计算成本，考虑增加正则化来代替减少模型规模。

这里还有两个额外的策略，在上一篇讨论解决偏差问题的文章中也有提到过，这里再重复一次：

- **基于误差分析，修改输入的特征**： 误差分析能够启发你新增新的特征，来帮助你的算法消除特定的错误类别。这些新的特征有助于减少偏差和方差。理论上，增加更多的特征会增加方差；如果你发现了这种情况，那么请使用正则化，这通常能消除方差的增加。
- **修改模型结构**（比如神经网络的结构）：这会让你的模型更适合你的问题：这能同时减少偏差和方差。

<br>

----------

> 上周，我们学习了一些技术来解决可避免偏差和方差。这周的文章会聚焦在学习曲线。它们是一种更具信息性和直观性的方法，可以帮助你找出有多少错误可以归因于可避免偏差或方差。
>   
> 请阅读这周的文章，学习更多（第28-30章）!

<br>

<div class="mk-toclify" id="markdown-toc-32"></div> 

# 学习曲线（Learning curves） 

<div class="mk-toclify" id="markdown-toc-33"></div> 

## 28 诊断偏差和方差: 学习曲线 

我们已经看到了一些方法可以用来估计有多少误差可以归因于可避免的偏差与方差。我们通过估算最优错误率和计算算法在training集和dev集的错误率来实现这一点。让我们来讨论一种更具信息化的技术：绘制一条学习曲线。

一条学习曲线描绘的是dev集上样本数量和误差之间的关系。为了绘制这个曲线，你可以在不同的样本数量上跑你的算法，得出不同的误差（错误率）。比如说，如果你有1000个样本，那么分别在100,200,300，...，1000的不同样本集大小上得到的错误率。这样你就可以绘制出学习曲线了。这里是一个例子：

![chapter_28_learning_curve.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_28_learning_curve.png)

随着training集的大小增加，dev集上的误差应该下降。

通常，我们会有一个“期望的错误率”，希望我们的算法能够最终实现。比如：

- 如果我们希望达到人的水平表现的话，那么“期望的错误率”就是人的出错率。
- 如果我们的算法是为了某些产品服务(比如发送猫的图片），为了给用户一个很好的用户体验，我们凭直觉为算法的性能定义一个必须满足的级别。
- 如果你在长期优化一个重要的应用，那么你应该可以凭直觉地知道你算法的性能在下个季度或者下一年能合理地进步多少。

在学习曲线图中增加“期望的错误率”：

![chapter_28_learning_curve_with_desire.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_28_learning_curve_with_desire.png)

你可以很直观地通过对红色的dev集错误率曲线的推断来猜测出，添加多少的样本数据，可以让你的算法的性能离“期望的错误率”还有多少距离。在上述这个例子，看起来增加2倍的样本数据，就可以让你算法的性能达到期望的性能（期望的错误率）。

不过如果dev集的误差曲线是直的就像高原平地的话，那么你也能立刻意识到不管添加再多的数据也无法提升算法的性能。

![chapter_28_learning_curve_with_desire_plateaued.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_28_learning_curve_with_desire_plateaued.png)

通过对学习曲线的观察，你可以避免花费几个月的时间在收集2倍的数据上，最后才意识到它没有任何帮助。

这个过程的一个缺点是，如果你只看学习曲线，随着样本数据的增加，很难推断和预测出红色曲线的确切位置。还有另外的一条曲线，可以帮助你预估增加样本数据带来的影响：training集的误差曲线。

<div class="mk-toclify" id="markdown-toc-34"></div> 

## 29 绘制training集的误差曲线 

你的dev集（test集）的误差应该随着training集的增大而减少。但是你的training集的误差通常会随着training集的增大而增加。

让我们用一个例子来说明这个效果。假设你的training集只有2个样本：一张猫图片和一张非猫图片。这样你的算法很容易“记住”这两张图片，取得了0%的错误。即使training样本中的任一个或者两个都被错误label了，算法仍然容易记住这两个label。

现在假设你的training集有100个样本。也许一些样本是标注错误，或者是模糊的-有些图片非常模糊，模糊到人也无法识别出它是否是猫图片。可能机器学习算法仍然可以“记住”大多数或者全部的training集数据，但很难达到100%的准确率。随着training集的大小从2个增长到100个，你会发现training集的准确率会略有下降。

最后，假设你的training集有10000个样本。在这种情况下，你的算法要完美地拟合近10000个样本就更难了，尤其是某些样本是模糊或者是错误标注的。因此，你的算法在training集上表现会更差。

![chapter_29_learning_curve_with_training.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_29_learning_curve_with_training.png)

你可以看到，蓝色的"training集误差曲线”随着training集的增大而增加。此外，通常你的算法在training上的表现要比dev集上要好。因此，红色的dev集误差曲线通常会严格地在蓝色的training集误差曲线以上。

下面我们来解释一下如何去解读这些曲线。

<div class="mk-toclify" id="markdown-toc-35"></div> 

## 30 解读学习曲线：高偏差 

假设你的dev集误差曲线如下图所示：

![chapter_30_learning_curve_with_interpreting_1.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_30_learning_curve_with_interpreting_1.png)

我们之前提过，如果你的dev集误差曲线一直都很平，那么你不太可能通过增加样本数量来达到预期的性能效果。

不过很难确切知道红色的dev集误差曲线继续延伸会是什么样子。如果dev集很小，你会更不确定，因为曲线可能是有噪声的。

假设你添加了training集误差曲线后：

![chapter_30_learning_curve_with_interpreting_2.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_30_learning_curve_with_interpreting_2.png)

现在，你可以绝对确信，仅仅是增加更多的样本数据是不够的。为什么？请记住我们的两个结论：

- 随着training数据的增加，training误差可能会变得更严重。因此，蓝色的training误差曲线只能不变或者更高，因此只会离期望的性能效果(绿色的曲线）越来越远。
- 红色的dev误差曲线一般会高于蓝色的training误差曲线。因此，当蓝色的training误差曲线在期望的性能效果线（绿色曲线）之上的时候，几乎没有办法通过添加更多的数据让红色的dev误差曲线下降，从而达到我们期望的性能效果。

在一张图表中同时查看dev误差曲线和training误差曲线，能够让我们更加推断dev误差曲线的走向。

为了讨论，假设期望的性能效果是我们估计的最佳错误率。上图就是标准“教科书”上所说的高可避免偏差的学习曲线：在training集数量最多的情况下-大概相当于使用了我们所拥有的全部的样本数据的情况下-training误差和期望的性能之前会存在一个很大的差距，意味着可避免偏差(avoidable bias)很大。此外，training误差曲线和dev误差曲线之间的差距很小，意味着方差很小。

在前面，我们只是通过图表中最右边的点，来测算training集和dev集的误差。这个点对应的是使用我们全部的样本数据。绘制完整的学习曲线可以使我们更全面地了解算法在不同训练集大小上的性能表现。

<br>

----------

> 这周的文章会教你如何阅读学习曲线。当你的数据集很小或者很大的时候，我也会给出一些实用的建议来教你如何在这种情况下画学习曲线。
>   
> 请开心地阅读第31-32章!

<br>

<div class="mk-toclify" id="markdown-toc-36"></div> 

## 31 解读学习曲线：其他案例 

考虑下图的学习曲线：

![chapter_31_learning_curve_with_high_var.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_31_learning_curve_with_high_var.png)

这个图表示的是高偏差？高方差？还是都是？

蓝色的training误差曲线相当低，红色的dev误差曲线比蓝色的training误差曲线要高得多。因此，偏差很小，但是方差很大。增加更多的训练数据可能会帮助减少dev误差曲线和training误差曲线之间的差距（减少方差）。

现在，考虑下图：

![chapter_31_learning_curve_with_high_bias_var.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_31_learning_curve_with_high_bias_var.png)

这一次，training误差相当大，比期望的性能曲线要高得多。dev误差同样也比training误差要大很多。因此，这同时有高偏差和高方差的问题。你得想办法同时降低偏差和方差。

<div class="mk-toclify" id="markdown-toc-37"></div> 

## 32 绘制学习曲线 

假设你的training集很小，只要100个样本。你以10为间隔递增地里面随机挑选出10个样本，然后是20个样本，再是30，最后到100个样本来训练你的算法。然后，你用这10个点来绘制你的学习曲线。你可能会发现，在较小的training集大小下，这条曲线看起来有点噪声（意味着某些点的值高于或者低于预期值）。

当你随机挑选10个样本来训练的时候，你可能很不幸地挑选了质量很差的样本，比如有很多模糊/标注错误。或者，你可能很幸运地挑选到了质量很好的样本。一个小的训练集，意味着dev和training误差曲线可能会随机波动。

如果你的机器学习应用严重倾向于一个类别（比如猫图片分类，负样本比例比正样本的要大很多），或者它有很多类别（比如识别的是100种不同的动物种类），那么选择出一个“特别不具备代表性”或者说质量很差的训练样本集的概率也会很大。比如，你的样本中有80%都是负样本（y=0），只有20%是正样本（y=1），这可能会导致随机挑选出来的10个样本全都是负样本，这意味着你的算法很难从中学习到有意义的东西。

如果你的training曲线有很多噪声，以至于你无法推断出真正的趋势，这里有两个解决办法：

- 不是只训练一个10个样本的模型，相反，有放回地从你100个原始样本集中随机挑选10个样本，重复几次（比如3到10次）。每次训练一个模型，计算每个模型的training和dev误差。计算出平均的training和dev误差，并且绘制成曲线。
- 如果你的training集倾向于只有一个类别，或者有很多类别，随机从100个样本里面选择一个“平衡的”子集作为训练集，而不是只挑选10个样本。比如，你可以确保子集中的2/10的样本是正样本，8/10的是负样本。更一般地说，你可以确保子集中的每一类的样本的比例尽可能地和原始的训练集的比例保持一致。

除非你已经尝试绘制学习曲线，并得出结论由于学习曲线噪声太多，看不到潜在的趋势，否则我不会去理会这些技术中的任何一种。如果你的training集很大-比如超过10000个样本-并且你的样本类别的分布不是很悬殊，你很可能并不需要这些技术。

最后，绘制一条学习曲线的计算成本可能很高。比如，你可能需要训练10个模型，从1000,2000，到10000个样本。训练小数据集的模型比大数据集的要快得多。因此，与其像上述一样线性地均匀平分training集的大小，不如分别用1000,2000,4000,6000和10000个样本来训练不同的模型，更加高效。这仍然可以让你清楚地了解学习曲线的趋势。当然，这个技术只有在训练模型的计算成本过高的时候才有意义的。

<br>

----------

> 学习算法在越来越多的领域中，从语音识别到图像识别（在某些限定领域），都已经超越了人的水平表现。和人的水平表现竞争，已经成为深度学习世界的一项新的运动，但同时也是许多企业很宝贵的努力。
> 
>  **为什么你应该利用人的水平表现来提高ML的发展呢？当你超过人的水平表现的时候会发生什么？**
>  
>  请通过阅读第33-35章找出答案!

<br>

<div class="mk-toclify" id="markdown-toc-38"></div> 

# 与人的水平表现对比（Comparing to human-level performance） 

<div class="mk-toclify" id="markdown-toc-39"></div> 

## 33 为什我们要与人的水平表现对比 

很多机器学习系统的目的就是使人类能做的事情自动化。例子包括图像识别，语音识别和垃圾邮件分类。许多机器学习算法已经提升了很多，以致于我们现在在越来越多的任务都超过了人的水平表现。

此外，针对人能表现很好的任务构建一个机器学习系统更加容易，原因有如下几个：

1. **更容易从标记者（human labelers）获取样本数据**。比如，因为人们能很容易地识别出猫的图片，一个很直观的做法是可以让人们提供高质量的猫图片标签数据。
2. **在误差分析过程中可以利用人类的直觉知识**。假设一个语音识别的算法比人类水平要差。比如说，它识别一个音频“This recipe calls for a ​pear ​ of apples”，误把“pear”识别成“paire”。你可以根据人的直觉知识，尝试去理解人要传递的信息，以此来修正机器学习算法。
3. **使用人的水平表现来估算最佳错误率和设置一个“可期望错误率”**。假设你的算法在某个人物错误率只有10%，不过一个普通人可以达到2%的错误率。这让我们知道最佳错误率是2%或者更低，可避免偏差至少是8%。因此，你应该尝试减少偏差的技术。

即使第三项听起来可能不是很重要，但我发现有一个合理的且可达到的错误率目标有助于项目的进展。了解你的算法有很高的可避免偏差是非常有价值的，它可以提供了一个可供尝试的技术选项列表去优化你的算法。

也有一些任务人类是不擅长的。比如，选择一本书推荐给你；或者是选择一个广告给用户展示；或者是预测股市。在这些任务，计算机已经超过了大多数人。对于这些任务，我们会遇到如下几个问题：

- **很难得到样本数据的label**。比如，标注者很难自动对用户数据库里的用户标记一个“最佳”的推荐书。如果你运营一个卖书的网站或者app，你可以通过向人们展示各种书以获得人们买书的记录作为样本数据的label。如果你没有运营这种网站，你需要通过更创新的方法去得到样本数据的label。
- **很难利用人的直觉知识**。比如，很可能没有人能预测股市。所以如果你的股市预测算法表现地比随机猜测还要差的话，你很难知道如何去改进它。
- **很难知道最佳错误率和可期望错误率是多少**。假设你已经有一个表现不错的书籍推荐系统。在没有人水平表现的基线的情况下，你怎么知道在它还能提高多少。

<div class="mk-toclify" id="markdown-toc-40"></div> 

## 34 如何定义人的水平表现 

假设你在开发一个可以通过x-ray图像自动诊断病情的医药图像项目。一个典型的没有医学背景只经历过基本培训的普通人能够得到15%的错误率，一个初级的医生能达到10%的错误率，一个资深的医生能达到5%错误率。一组医生对图像经过讨论和辩论能达到2%错误率。哪个错误率能代表“人的水平表现”？

在这个例子，我会使用2%作为人的水平表现和最佳错误率。你也可以把2%当做为期望错误率的估计，原因是在上篇文章中提到的3个原因：

1. **更容易从标记者（human labelers）获取样本数据**。你可以组建一组医生为你提供错误率只有2%的样本标签。
2. **在过程中可以利用人类的直觉知识**。在一组医生中对图像的讨论过程中，你可以推断出他们的直觉经验知识。
3. **使用人的水平表现来估算最佳错误率和设置一个“期望错误率”**。使用2%作为最佳错误率的估计是合理的。最佳错误率可能还会比2%要低，但不可能更高。因为一组医生是有可能达到2%的错误率的。相反，把5%或者10%作为最佳错误率的估计是不合理的，因为我们知道这两个估计值太高了。

当我们真正要收集样本数据的时候，因为一组医生的时间成本太高了，我们不太可能让他们对每张图像都诊断。可能我们让一个初级医生对大多数的图像做诊断，把很难诊断的图像留给资深医生或者一组医生。

如果你的系统的错误率已经是40%了，那么不管你是让初级医生（10%错误）还是资深医生（5%错误）标注你的样本都不重要了。不过如果你的系统的错误率达到了10%，那么把人的水平表现定义为2%，能提供更好的工具来改进你的系统。

<div class="mk-toclify" id="markdown-toc-41"></div> 

## 35 超越人的水平表现 

假设你现在在开发一个语音识别系统，并且有一个音频数据集。假设你的数据集有很多高噪声的音频，以至于人只能达到10%的错误率。假设你的系统达到了8%的错误率。你可以用在第33章提到的3种技术的任意一种，来加速你的开发过程吗？

如果你能识别出人类显著超过你的系统的数据子集，那么你仍然可以使用那些技术来推动项目的快速发展。比如，假设你的系统比人类更擅长识别噪声音频，但是人类更擅长转译那些语速很快的音频。

对于那些语速很快的音频：

1. 你仍然可以获得人们转译的结果作为样本数据，因为它的质量比你算法的输出要高。
2. 你可以可以利用人的直观经验知识来理解为什么人能够正确理解语速很快的音频，但你的系统却不能。
3. 你可以把人在语速很快的音频上的的水平表现作为预期性能和目标。

一般地说，只要你的dev集存在人们能够正确处理，而你的算法不能的数据，前面描述的很多技术都可以应用上，即使你的算法在整个dev/test集上的平均性能，已经超过了人的水平表现。

很多重要的机器学习系统已经超过了人的水平表现了。比如，机器更能预测电影收视率，卡车开到某个地方的所需时间，或者是否批准某个贷款申请。一旦人也很难识别算法明显出错的例子，那么只剩下一小部分的技术有用处。 因此，在机器已经超越人类水平的问题上，进步通常比较缓慢，而当机器仍在追赶人类时，进步会更快。

<br>

----------

> 我们在获取更多的数据集来训练我们的猫图片检测器，erm.，学习算法。但是，当你的training数据和test集的数据不是同一分布的时候，会产生什么问题呢？ <br> **在这周，你将会学到：** <br> 
> 

> - 什么时候你应该收集和你test集不同分布的training集数据。
> - 当你有不同类型（分布）数据的时候，如何切分train/dev/test集。
> - 如何判断在training中包含不同（分布）的数据是否是一个好主意的。

>  请开心地阅读！请阅读第36-39章 

<br>

<div class="mk-toclify" id="markdown-toc-42"></div> 

# 不同分布的训练和测试 

<div class="mk-toclify" id="markdown-toc-43"></div> 

## 36 什么时候你应该在不同分布上训练和测试 

用户通过你猫图片app上传了10000张图片，你对这些图片已经标注了哪些有猫，哪些没有。同时，你从互联网上下载了一个20万张更大的图片集。这时候，你应该怎么定义你的training/dev/test集。

因为你的10000张用户图片已经基本能反映你的数据的真实的分布情况，你可能会直接当做dev和test集使用。如果你的算法是一个真值数据稀少的深度学习算法，你可能会把从网上下载的额外的20万张图片作为training集。这样，你的training集和dev/test集的数据服从不一样的概率分布。这种情况会对你的工作有什么影响？

相对于把我们的数据按照如上方法分隔成train/dev/test集，我们也可以把所有的21万的图片合在一起打乱，再随机切分为train/dev/test集。这种情况下，所有的数据都来自同一分布。但我反对这种方法，因为大概97.6%（205000/210000)的dev/tets集数据来自互联网，并不真正反映真实数据的分布情况。请记住我们的构造dev/test集的建议：
构造的dev/test集的样本数据要能反映你未来希望得到并处理的数据

机器学习中的大多数学术文献都假设train集、dev集和test集都来自同一分布。*(有一些学术研究如何在不同的分布进行train和test。例子包括领域自适应、迁移学习、多任务学习。但理论和工程之间还是有巨大的差距。如果你在数据集A上训练，但在某些与A非常不一样的数据集B上测试，你的算法表现有多好几乎全凭运气。（这里的“运气”包括研究员人工设计的特征，还有一些其他我们还不了解的因素。这使得对不同分布的train和test的理论研究难以系统地进行）*。在机器学习早期，数据很缺乏。我们通常只有一个来自某个概率分布的数据集。因此我们可以随机地把数据切分为train/dev/test集合，并且，所有数据来自同一分布的假设条件通常都是满足的。

不过在大数据时代，我们可以获得庞大的training集，比如互联网的猫的图片。即使training集和dev/test集不是来自同一分布，我们仍然希望能够使用这些数据，因为它可以提供很多信息。

在猫图片识别系统的例子，相比我们把所有的1万张用户上传的图片切分为dev/test集，我们只把5000作为dev/test集。我们把剩余的5000张放入到training集。按照这种方法，你的training集包含了20.5万个样本,其中0.5万和你的dev/test集来自同一分布，另外的20万来自互联网图片。我们在下一章章节讨论为什么这种方法是有效的。

让我们考虑第二个例子。假设你在构建一个语音识别系统，为一个语音控制的地图导航软件提供地址语音到文本的转译功能。你有2万个用户的口述街道地址的音频样本数据。但是你也有50万个其他的与地址无关的音频样本数据。你可能会把1万个用户口述地址的音频样本作为dev/test集，把剩下的1万，加上另外的50万个样本，作为training集。

我们会继续假设你的dev和test集数据来自同一分布。但是重要的是，我们要能理解不同分布的train和dev/test集会引发一些特殊的挑战。

<div class="mk-toclify" id="markdown-toc-44"></div> 

## 37 如何决定是否使用所有的数据 

假设你的猫图片识别系统的training集包含了1万张用户上传的图片。这些数据与单独的dev/test集有着相同的分布，代表着你所关心的真实数据的分布。你同时有额外的2万张来自互联网的图片。你是应该把所有的2万+1万=3万张图片作为training集，还是应该因为担心导致算法有偏而丢弃掉这2万张互联网图片呢？

当使用的是早期的机器学习算法（比如手工设计的计算机视觉特征，然后是一个简单的线性分类器）时，合并这两类数据是有一定的风险会导致你的算法表现地更差。因此，有些工程师会警告你不要使用那2万张互联网图片。

但在强大的、灵活的机器学习算法时代（如大规模神经网络），这种风险已经大大降低。如果你能够建立一个有着数量足够多的隐藏神经元/层的神经网络，你可以安全地将20000张图片添加到你的training集。添加图片更有可能提高你的算法的性能。

这种观察依赖于这个事实，有一些x->y的映射在两种类型的数据都表现地很好。换句话说，存在一些这样的系统，输入不管是来自互联网还是手机APP图片都可以很可靠地预测出其标签，即便这个系统不知道这些图片的来源。

增加这些额外的2万张图片有如下的影响：

1. 它让你的神经网络有了更多的是猫或者不是猫的样本。这是有帮助的，因为互联网图片和用户上传的图片是有着某些相似性。你的神经网络能够把从互联网图片学习到的知识应用在手机APP图片上。
2. 它迫使神经网络花费一些能力来学习特定于互联网图片的属性（比如更高的分辨率，图片帧的不同分布等）。如果这些属性与手机APP图片有很大的不同，它将“耗尽”神经网络的一些表达能力。因此，这个神经网络从手机APP图片的分布中识别数据的能力会偏弱，这才是你真正关心的东西。理论上，它能损害算法的性能。

为了用不同的术语来描述第二个影响，我们可以求助于虚构的人物夏洛克·福尔摩斯，他说你的大脑就像阁楼，它只有有限的空间。每当要增加知识的时候，你就会忘记一些以前知道的知识。因此，最重要的是不要用无用的知识来排挤掉有用的知识*（来自于阿瑟·柯南·道尔在《Scarlet》的研究）*。

幸运地是，如果你有足够的计算能力来建立足够大的神经网络-一个足够大的阁楼-那么这不会成为一个你要密切关注的严重问题。足够大的神经网络可以有足够的能力从互联网和收集APP的图片学习到到足够的知识，而不用这两类数据分别争夺它的表达能力。你的算法的“大脑”足够大，你不必担心用尽你的阁楼空间。

不过如果你没有一个足够大的神经网络（或者其他高灵活性的机器学习算法），那么你应该要更加注意你的training集的数据要与dev/test集来自同一分布。

如果你觉得某些数据没有丝毫作用，那么为了计算成本，你应该直接去掉这些数据。比如，假设你的dev/test集主要包含人物、地点、地标建筑和动物的日常照片。假设你也有大量的历史文档的扫描图片：

![chapter_37_document_scanned.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_37_document_scanned.png)

这些文档图片没有包含任何和猫相似的东西。它们看起来也完全和你的dev/test集不一样。因为第一条影响的收益可以忽略不计，神经网络从中几乎学习不到什么可以应用在dev/test集的东西，因此没有任何理由把这些数据当做负样本。这些数据反而会浪费计算资源和神经网络的表达能力。

<div class="mk-toclify" id="markdown-toc-45"></div> 

## 38 如何决定是否要包含不一致数据 

假设你要预测纽约的房价。给定房子的大小（输入特征x），你希望预测出它的价格（目标label y）。

纽约的房价非常高。假设你有第二个数据集，是底特律的房价，比纽约的房价要低得多。你应该把它们放入到你的training集吗？

给定同样的房子大小，在纽约还是底特律会让房子的价格有很大的区别。如果你只是想要预测纽约的房价，把这两个数据集合在一起，会影响你的算法性能。在这个例子，最好把底特律这个不一致的数据集丢掉。
*（有一个方法可以解决底特律和纽约数据不一致的问题，就是增加一个额外的特征表现在哪个城市。给定输入特征x-现在包含了城市特征了-目标值y的预测现在变得明确了。然而，在实践中，我并不经常看到这种情况。）*

纽约和底特律的例子与手机APP和互联网猫图片的例子有什么不同吗？

猫图片的例子不同的地方在于，人们能清楚地识别出这张图片是否有猫，而不用知道它是互联网图片还是手机APP图片，也就是说，有一个函数f（x）能可靠地从输入x映射到输出y，而不用知道x的来源。因此识别互联网图片的任务和识别手机APP图片的任务是“一致的”。这意味着包含这些所有的数据几乎没有什么缺点（除了计算成本），反而可能有显著的好处。相反，纽约的房价数据和底特律的房价数据是“不一致的”。给定同样的x（房子的大小），房子在哪个城市会导致房价有很大的区别。

<div class="mk-toclify" id="markdown-toc-46"></div> 

## 39 给数据赋予不同权重 

假设你有20万张来自互联网的图片和5000张来自手机APP用户的图片。这两个数据集的大小比例是40:1。理论上，只要你能基于这所有的20.5万张图片建立一个大规模的并且训练足够多时间的神经网络，这个问题不会影响到你。只要你建立一个规模庞大的神经网络，并且在所有20.5万张图片上有足够长时间的训练，就可以使算法在互联网图片和手机APP图片上都能表现地很好。

但在实践中，相比只训练5000张图片，增加40倍的数据量意味着增加40倍（或者更多）的计算资源。

如果你没有巨大的计算资源，作为妥协，你可以给互联网图片样本赋予一个低得多的权重。

比如，假设你的优化目标是均方误差（对于分类任务，它不是一个好选择，但它可以简化我们的解释。），因此，我们的算法会尝试去优化如下的公式：

![chapter_38_optimize_objective_1.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_38_optimize_objective_1.png)

上述公式中，第一个求和是对5000张手机图片，第二个求和是对20万张互联网图片。你可以增加一个额外的β参数来优化：

![chapter_38_optimize_objective_2.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_38_optimize_objective_2.png)

如果你把β设为1/40，你的算法会认为5000张手机照片和20万张互联网图片拥有同样的权重。你也可以通过对dev集的调优，把β设为其他值。

通过对互联网图片赋予更低的权重，你不用建立一个足够庞大的必须同时在这两类图片数据上都表现很好的神经网络。只有当你怀疑额外的数据（互联网图片）和dev/test集具有非常不同的分布时，或者分布一样，但这种额外数据的数据量远比dev/test集（手机图片）大的多的时候，才需要进行这种类型的重新加权。


<br>

----------

> 上周，我们讨论了当你的training集和dev/test集数据不是同一分布会产生的问题。你怎么知道你的算法的问题是否就是不能很好地泛化到不同分布的数据呢？ 我们把这个问题叫做**数据不匹配（data mismatch)**。<br> 在这周，你将会学习到如何诊断出算法的问题是否是数据不匹配，而不是偏差和方差。一旦你确定是数据不匹配的问题，你将会学习解决数据不匹配问题的一些技术，比如**人工数据合成（artificial data synthesis）**。继续阅读！ <br> 请阅读第40-43章 

<br>

<div class="mk-toclify" id="markdown-toc-47"></div> 

## 40 从training集到dev集的泛化 

假设你在training集和dev/test集分布一致的情况下优化你的ML算法。具体地说，training集包含了互联网图片+手机APP图片，dev/test集只包含了手机APP图片。然而，算法的表现很差：dev/test集上的误差比你希望的要差得多。以下是一些可能的原因问题：

1. 算法没有拟合好training集。这是一个training集上的高偏差的问题。
2. 算法很好地拟合了training集，但没有很好地泛化到之前没见过的*与training集有相同分布的*数据。这是一个高方差的问题。
3. 算法很好地地泛化到*与training集有相同分布的*新数据，但不能很好地泛化到那些只和dev/test有相同分布的新数据。我们把这个问题称为**数据不匹配（data mismatch）**，这是因为training集的数据和dev/test集的数据匹配程度很低。

比如，假设人在猫图片识别任务中能取得近乎完美的表现。你的算法的指标如下：

- 在training上错误率为1%
- 在与training集有相同分布的的新数据上的错误率为1.5%
- 在dev/test集上错误率为10%

在这种情况下，很显然，你遇到了数据不匹配（data mismatch)问题。为了解决这个问题，你可能要尝试让training集的数据与dev/test集更相似。我们会在后面讨论一些相关的技术。

为了诊断出算法分别受到上述3个问题多大程度的影响，我们建立另一个数据集。具体地说，不是把所有的training数据都拿来训练，你可以把它分为两个子集：真实的用来训练的training集，一个单独的数据集，我们将之叫做“training-dev集”，这个数据集并不会拿来训练。

现在你有4个数据集：

- training集。这是真实用来训练的数据（互联网图片+手机图片）。这些数据不必和我们真正关心的数据(dev/test集)有着相同的分布。
- training-dev集。这些数据和training集（互联网图片+手机图片）数据有相同的分布。它通常比training集要小。它只需要足够大到能评估和跟踪我们的算法改进就可以。
- dev集。和test集有相同的分布，它反映着我们最终真正关心的数据的分布情况(比如手机图片）。
- test集。和dev集有相同的分布。

有了这4个数据集，你可以评估出：

- training集的错误率。
- training-dev集的错误率。表示算法在training集训练得到的对于同一分布的新数据的泛化能力
- dev/test集的错误率。表示算法在你真正关心的数据上的性能。

第5-7章中的用于选择dev集大小的大部分准则也适用于training-dev集。

<div class="mk-toclify" id="markdown-toc-48"></div> 

## 41 识别偏差、方差和数据不匹配导致的误差 

假设人在猫图片识别任务中能取得近乎完美的表现（约为0%错误），那么最佳错误率大概是0%。假设你有如下指标：

- training集的错误率为1%
- training-dev集的错误率为5%
- dev集的错误率为5%

这表示什么？这里，你知道这是一个高方差问题。上述章节中减少方差的技术能够帮助你改进你的算法。

现在，假设你的指标如下：

- training集的错误率为10%
- training-dev集的错误率为11%
- dev集的错误率为12%

这告诉你，你的算法在training集上遇到了高偏差的问题。也就是说，你的算法在training集上表现很差。减少偏差的技术应该能够帮助你。

在上述两个例子，你的算法只是遇到了高偏差或者高方差其中之一。有可能你的算法同时遇到了高偏差、高方差和数据不匹配之中的任意组合。比如：

- training集的错误率为10%
- training-dev集的错误率为11%
- dev集的错误率为20%

算法同时遇到了高偏差和数据不匹配的问题。然而，它在training集上并没有高方差问题。

通过将它们展示在表格中，我们可以更容易理解不同类型的错误是如何相互关联的：

![chapter_41_error_table.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_41_error_table.png)

继续我们的猫图片识别系统的例子，表格x轴有两列表示不同的数据分布，y轴有三行，表示三种类型的错：人类水平表现错误，训练集上的错误，非训练集上的错误。我们把刚才提到的不同类型的错误率填入到这个表格之中。

如果你愿意，你也可以把表格的其余两个单元格也填了。你可以让人标注手机APP的猫图片得到他们的错误率，填入右上角的单元格。你可以把手机APP猫图片（Distribution B）的一小部分放入到training集中，这样神经网络可以学习到它们。然后你计算出在那部分的数据上的错误率，填入旁边的单元格。填了这两个单元格的数据后，有时候可以让我们额外认识到关于算法在两个不同分布（Distribution A和B）下的表现。

通过理解哪种错误对你的算法的影响是最大的，你可以更好地决定是否专注在减少偏差、减少方差还是减少数据不匹配问题。

<div class="mk-toclify" id="markdown-toc-49"></div> 

## 42 解决数据不匹配问题（data mismatch）

假设你的语音识别系统在training集和training-dev集上表现都非常好。但是，在dev集上表现很差：你遇到了数据不匹配问题。你应该怎么做？

我建议你：（i）尝试去了解training集和dev集的数据有哪些不一样的属性。（II）尝试去收集更多与dev集更匹配的training数据。*（这也被称为“领域自适应”-如何在一个分布的数据上训练算法，并将其推广到不同分布的数据。，关于领域自适应也有一些研究。这些方法通常只适用于特殊类型的问题，远不如本章中所描述的方法被广泛使用。）*

比如，假设你在dev集上开展误差分析：你人工查看了100个样本，尝试找出算法误判的的原因。你发现你的算法表现差是因为dev集的音频样本大多数是在汽车内录制的，而training集的大多数样本都是在安静的环境下录制的。在这种情况下，你可以尝试去获取更多在汽车内录制的音频样本作为training数据。误差分析的目标是去找出training集和dev集数据上的不同之处，这些不同之处导致了数据不匹配的问题。

如果你的training和training-dev集都包含在汽车内录制的音频样本，你应该仔细检查一下你的系统在这些数据子集上的性能表现。如果你的系统只在training中汽车内录制的音频样本上表现很好，却在training-dev中的汽车内录制的音频样本上表现很差，这进一步验证了，获得更多的汽车内录制的音频样本是有帮助的。这就是为什么我们在前面章节中讨论了把dev/test集上同一分布的数据引入到training集的可能性。这样做可以让你在training集和dev/test测试集上的汽车内录制的音频样本上，同时比较你的算法的表现，

不幸的是，这个过程不一定保证可行。比如，假如你没有办法获得更多的与dev集更匹配的training数据，你可能没有明确的方法来提高算法的性能表现。

<div class="mk-toclify" id="markdown-toc-50"></div> 

## 43 人工数据合成（artificial data synthesis）

你的语音系统需要更多的听起来好像是在车内录制的音频样本数据。与其在开车时收集大量音频数据，我们有更简单的方法来获得这些数据：人工合成。

假设你有大量的汽车/道路噪声的音频。你可以从一些网站下载到这些数据。假设你也有人们在安静房间说话的大量音频。如果你把人们安静说话的音频加入到汽车/道路噪音频中，你可以得到一个听起来好像人在嘈杂的汽车内说话的音频数据。通过这个方法，你可以“合成”大量的听起来像是在汽车内收集到的样本数据。

一般地说，有几种情景，允许你通过人工数据合成的方法，生产大量的和dev集合理匹配的样本数据。让我们用猫图片识别系统作为第二个例子。你发现dev集的图片有更多运动的模糊，因为它们来自于手机用户，而手机用户拍照的时候会稍微移动他们的手机。你可以把training集中清晰的互联网照片，加入一些模拟的运动模糊，从而使它们更像dev集的照片。

注意，人工数据合成有它的挑战：有时候，创建对人看起来很逼真的数据，比创建对计算机看起来很逼真的数据，要更容易。比如，假设你有1000小时的音频训练数据，但是只有一个小时的汽车噪声数据。如果你重复使用那同样的1小时的汽车噪声数据，和原来那1000小时的音频训练数据中的不同部分，一起合成人工数据，你最终会得到一个重复出现相同的汽车噪声的合成数据集。当人听这些音频数据，他们可能发现不了-所有的汽车噪声对我们来说听起来没什么区别-但是机器学习算法可能会“过拟合”这个1小时的汽车噪声。因此，遇到那些汽车噪声听起来不一样的新音频的时候，你的算法就会表现地很差。

或者，假设你有1000个小时不同的汽车噪声，但是都是来自于10俩汽车。在这种情况下，你的算法可能会“过拟合”这10俩汽车，导致在其他汽车上表现很差。不幸的是，这些问题很难被察觉出来。

![chapter_43_car.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_43_car.png)

再多举一个例子，假设你在开发一个识别汽车的计算机视觉系统。假设你和一家视频游戏公司合作，它有几款汽车的计算机图形模型。为了训练你的算法，你用这些模型来生成汽车的合成图片。即使这些合成图片十分逼真，这个方法(这个方法已经被很多人独立地提出）可能不会凑效。这个视频游戏可能只有大概20种汽车的设计。构建一个3D的汽车模型是很耗时的。如果你在玩这个游戏，你可能不会注意到你多次看到了同一辆车，也许只是油漆不同。也就是说，这些数据对你来说非常逼真。但相对于路边的所有汽车-可能就在你的dev/test集-这20俩汽车在整个汽车世界的分布中只占了非常小的部分。因此，如果你的10万个训练样本全部来自于这20俩汽车，你的系统会“过拟合”这20俩汽车的设计，它不能很好地泛化到包含其他汽车设计的dev/test集。

当合成样本数据的时候，请考虑一下你合成的是否是有代表性的样本。尽量避免给出合成数据属性，从而避免算法可以区分出合成和非合成的样本，例如所有的合成数据都来自20俩汽车设计的某一个，或者所有的合成音频都只来自那1个小时的汽车噪声。实际上，这个建议是很难付诸实施的。

在进行数据合成时，为了产生显著的效果，我的团队有时会花费数周的时间来产生数据，产生细节足够接近真实分布的合成数据。如果你能把细节都弄得很正确，你可以瞬间获得远比以前大的多的training集。

<br>

----------

> 用于调试语音识别系统、机器翻译系统和强化学习系统的AI的设计模式都有哪些？ <br> 我和我以前的博士生Pieter Abbeel和Adam Coates共同开发了一个直升机自动驾驶的强化学习项目，在这个项目中，我学习到一个技巧，这个技巧有助于改善所有的这些AI系统。继续阅读！ <br> 请阅读第44-46章 

<br>

<div class="mk-toclify" id="markdown-toc-51"></div> 

# 调试推理算法（Debugging inference algorithms）

<div class="mk-toclify" id="markdown-toc-52"></div> 

## 44 优化验证试验

假设你在开发一个语音识别系统。你的系统输入一段语音A，输出可能的句子S和它们的分数score-A(S)。比如，你可能尝试去计算Score-A(S)=P(S|A)。表示输入语音是A的条件下，S是正确句子的概率。

有了计算Score-A(S)的方法，你仍然需要找到最大概率的句子S：

![chapter_44_output_probability.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_44_output_probability.png)

你应该怎么找出“最大概率”？如果英语有5000个单词，那么长度为N的句子理论上有50000的N次方个可能的组合-多到无法穷尽。所以，你需要一个近似的搜索算法，去找到令Score-A(S)最大的句子S。有一种搜索算法叫“beam search”，在搜索过程中，它只保持最好的K个候选句子。（基于本章的目的，你不需要理解beam search的细节)。类似这样的算法并不保证一定会找到令Score-A(S)最大的句子S。

假设有一段音频A，记录着某人说“I love machine learning.”。但你的系统没有输出正确的句子，而是输出“I love robots.”。有两个可能的原因导致系统出错：

1. **搜索算法问题**。 搜索算法（bean search）没有找到那个令Score-A(S)最大的句子S。
2. **目标函数（评分函数）问题**。我们的评分函数Score-A(S)=P(S|A)不准确。也就是说，我们的评分函数Score-A(S)没有识别出“I love machine learning.”是正确的句子。 

根据这两个不同的原因，我们应该采用不同的解决方法。如果是原因1，你应该去优化搜索算法，如果是原因2，你应该去优化评分函数Score-A(S)。

面对这种情况，一些研究员会随机地决定优化随机算法，另外一些研究员会随机地去优化评分函数Score-A(S)。但是除非你明确知道哪个原因引起了这个错误，否则你只会白费努力。你怎样才能更系统性地决定下一步优化什么？

让Sout是你的系统输出的句子（“I love robots“）让S\*是正确的句子（“I love machine learning”）。为了去找出这个错误是原因1还是原因2导致的，你可以执行一个**优化验证试验（Optimization Verification test）**：首先，计算Score-A（S\*）和Score-A(Sout)，然后检查Score-A（S\*）和Score-A(Sout)的大小。结果有两个可能性：

Case 1：Score-A（S\*）> Score-A(Sout)

这种情况下，你的算法正确地把S\*赋予了比Sout更高的分数。尽管如此，你的搜索算法还是选择了Sout，而不是S\*。这意味着你的搜索算法不能找到令Score-A(S)最大的句子S。这种情况下，优化验证试验（Optimization Verification test）告诉你，你遇到的是一个搜索算法的问题，你应该集中精力解决它。比如，你可以尝试增加beam search的宽度(beam width)。

Case 1：Score-A（S\*）<= Score-A(Sout)

在这种情况下，你可以知道，Score-A(.)的计算有问题：它错误地把S\*赋予了比Sout更低的分数。优化验证试验（Optimization Verification test）告诉你，你遇到的是一个目标函数（评分函数）问题，你应该集中精力在如何提高Score-A(.)的计算准确度。 

我们上面说的只是一个单独样本。在实践中，为了应用优化验证试验，你应该对dev集的所有错误进行检查。对于每个错误，你都应该检查是否Score-A（S\*） > Score-A(Sout)。如果不等式成立，则标记为搜索算法的问题。如果不成立，则标记为目标函数（评分函数）问题。

比如，假设你发现95%的错误是因为评分函数Score-A(.)的问题，只有5%是因为搜索算法的问题。现在，你应该知道不管你如何优化你的搜索算法，最理想的情况下你只能减少大概5%的错误。那么，你应该去提高Score-A(.)的计算准确度。

<div class="mk-toclify" id="markdown-toc-53"></div> 

## 45 优化验证试验的一般形式

当这种问题：给定一些输入x，你知道如何计算一个分数score-x(y)，表示在输入x的情况下，输出的y有多好，你可以应用优化验证试验的技术。此外，你使用一个近似的算法找出arg-max（score-x(y)）中的y，但搜索算法有时候找不到这个最大值的y。在我们之前的语音识别的例子，x=A，表示音频数据，y=S表示输出的句子。

假设y\*是正确的输出，但算法给出的是yout。那么关键的试验是检查是否Score-x（y\*） > Score-x(yout)。如果不等式成立，我们归因于搜索算法的问题。请参考前一章以确保理解这背后的逻辑。否则，我们归因于Score-x(.)的计算问题。

让我们再看一个例子。假设你在开发一个中文到英文的机器翻译系统。你的系统输入一个中文句子C，然后对于每个可能的英文句子E计算对应的分数Score-c(E)。比如，你可以使用Score-c(E)=P(E|C)，表示输入句子是C的条件下，E是正确翻译的句子的概率。

你的算法通过计算如下的公式来翻译句子：

![chapter_45_output_probability.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_45_output_probability.png)

然而，可能的英文句子E的集合太大了，所以你依赖一个启发式的搜索算法。

假设你的算法输出一个不正确的翻译句子E-out，而不是正确的翻译句子E\*。那么优化验证试验要求你检查是否Score-c（E\*） > Score-c(E-out)。如果不等式成立，表示Score-c(.)正确地把E\*排在E-out之前；因此，我们归因于搜索算法的问题。否则，我们归因于Score-c(.)的计算问题。

这是AI领域中一个非常通用的“设计模式”，先学习一个近似的评分函数Score-x(.)，然后使用一个近似的最大化算法。如果你能够理解这种模式，你将能够使用优化验证试验来了解您的错误的来源。

<div class="mk-toclify" id="markdown-toc-54"></div> 

## 46 强化学习实例

![chapter_46_reforcement_learning.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_46_reforcement_learning.png)

假设你在使用机器学习算法来教直升机学会复杂的飞行动作演习。上图是一组延时照片，拍摄的是一架由计算机控制的直升机在发动机关闭的情况下降落的过程。

这被称为“自转”演习。它允许直升机降落，即使他们的发动机引擎发生了故障。人类飞行员把“自转”演习作为训练的一部分。你的目标是使用一个学习算法让直升机可以沿着一个能让它安全着陆的轨迹T飞行。

为了实现这个强化学习算法，你得开发一个“Reward函数” R(.)。这个函数通过一个分数给出每一个可能的轨迹T有多好。比如，如果T导致直升机坠毁了，那么reward可能是R(T)=-1000--一个很大的负值。如果T让直升机安全着陆了，那么R(T)是一个正值，大小取决于降落过程的平稳程度。reward函数通常由人为设定，以量化不同的轨迹T的期望程度。它必须权衡着陆的颠簸程度，直升机是否降落在所需的地点，乘客的乘坐感受，等等。设计一个好的reward函数不是一件容易的事情。

给定一个reward函数R(T)，强化学习算法的任务就是控制直升机，让它飞行轨迹的R(T)的值最大。然而，强化学习算法做了很多近似，可能并不能成功地实现这个最大值。

假设你已经训练好了某个reward函数R(T)，然后已经跑完你的强化学习算法。然而，算法的表现远比人类飞行员要差-着陆过程更崎岖，看起来更不安全。你怎么能知道是你的强化学习算法（尝试找出最大化R(T)的轨迹T）有问题，还是reward函数（对着陆的颠簸程度和着陆点的精准度，给出综合的打分）有问题？

为了实现优化验证试验，把T-human定义为人类飞行员的飞行轨迹，T-out是算法给出的飞行轨迹。根据我们上述的描述，T-human的轨迹比T-out更优。因此，关键的试验是，是否R(T-human) > R(T-out)?

Case 1：如果不等式成立，那么reward函数R(.)正确地把T-human排在了T-out前面。但是我们的强化学习算法找到的却是更差的T-out。这表明改进我们的强化学习算法是值得的。

Case 2：如果不等式不成立：R(T-human) <= R(T-out)。这表明，R(.)给T-human了较低的分数，即使它是更好的轨迹。你应该改进R(.)函数，以便它更好地体现一个好的着陆过程。

很多机器学习算法都通过这种“模式”来使用一个近似的搜索算法来优化评分函数Score-x(.)。有时候，没有指定的输入x，那么问题就变成只优化Score(.)了。在我们上述的例子，评分函数就是reward函数Score(T)=R(T)，搜索算法是强化学习算法，尝试执行一条好的轨迹T。

这个例子和之前的例子的一个不同点在于，你不是和一个“最优”的输出对比，而是和一个人的水平表现输出T-human对比。我们假设T-human即使不是最优也是相当好。一般来说，只要你有比你当前的学习算法更优的输出y\*（在这个例子，是T-human）--即使y\*不是“最优”的输出，那么优化验证试验（Optimization Verification test）可以表明在现阶段，改进算法和改进评分函数，哪个更有价值。

<br>

----------

> 我曾经帮助构建了一个巨大的端到端的语音识别系统，叫深度语音，它可以很好地完成语音识别的端到端学习。但是，尽管许多团队在端到端的深度学习方面取得了巨大的成就，但这样的系统并不总是一个好主意。<br> 什么是端到端的深度学习？你应该什么时候使用它，什么时候应该避免使用呢？继续阅读，找出答案！ <br> 请阅读第47-49章 

<br>

<div class="mk-toclify" id="markdown-toc-55"></div> 

# 端到端的深度学习

<div class="mk-toclify" id="markdown-toc-56"></div> 

## 47 端到端深度学习的兴起

假设你希望构建一个系统来检查网上产品的评论，自动识别出这个评论者是喜欢还是不喜欢这个产品。比如说，你希望能识别出如下的评论是正向的评论：

This is a great mop!

如下的评论是负向的评论：

This mop is low quality--I regret buying it

识别正向或者负向评论的问题，我们称之为“情感分类”。

构建这个系统，你可以构建这两个组件的一条“pipeline”：

1. Parser: 对文本进行标注，识别最重要的单词*（一个Parser能够标注出远更丰富的内容，但是这里简化的描述足以解释端到端的深度学习）*。比如，你可以用Parser标记出所有的形容词和名词。因此，你可以得到下面的标注文本：

![chapter_47_end-end_annotated_text.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_47_end-end_annotated_text.png)

2. 情感分类器（Sentiment classifier）：一个学习算法，输入标注后的文本，预测出整体的情感类别。Parser的标注可以极大地帮助到这个算法。通过给形容词一个更高的权重，你的算法将能够很快就聚焦在一些重要的词，如“great”，而忽略那些不重要的词，如“this”。

如下图，我们可以可视化这两个组件的的“pipeline”：

![chapter_47_end-end_pipeline.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_47_end-end_pipeline.png)

近期有种趋势，用一个单一的学习算法来取代这个“pipeline”系统。**端到端的学习算法**可以简单地输入原始的文本“This is a great mop!”，并尝试直接识别出它的情感类别。

![chapter_47_end-end_pipeline_end_2_end.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_47_end-end_pipeline_end_2_end.png)

神经网络常用于端到端的学习系统中。“端到端”这个词表明，我们要求算法直接从输入得到期望的输出。也就是说，算法直接连接着“输入端”到“输出端”。

在那些数据丰富的问题中，端到端的学习系统已经取得了相当的成功。但它们并不总是一个好的选择。接下来的几章，我们会通过更多的端到端的例子给出建议，什么时候应该使用端到端的学习系统，什么时候不应该使用。

<div class="mk-toclify" id="markdown-toc-57"></div> 

## 48 更多端到端学习的例子

假设你要构建一个语音识别系统。你可能会构建如下三个组件：

![chapter_48_end_end_more_example_speech.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_48_end_end_more_example_speech.png)

这些组件的工作方式如下：

1. Compute features：抽取人工设计的特征，比如MFCC（​Mel-frequency cepstrum coefficients）特征，试着捕捉说话的内容，而忽略不相关的属性，比如说话的音高。
2. Phoneme recognizer：一些语言学家认为，声音的基本单位叫做“音素”。例如，“keep”中的初始“K”音与“cake”中的“C”音的音素相同。这个组件尝试识别出音频片段中的音素。
3. Final recognizer：接收识别出的音素的序列，尝试把音素序列转为输出文本。

相反，一个端到端的系统可能输入的是一个音频片段，直接输出转译的文本。

![chapter_48_end_end_more_example_speech_2.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_48_end_end_more_example_speech_2.png)

至今，我们只描述了完全线性的“pipelines”的机器学习系统：输入顺序地从一个阶段传递给下一个阶段。"pipeline"可以更复杂。比如，下图是自动驾驶汽车的一个简单的架构图：

![chapter_48_end_end_more_example_car.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_48_end_end_more_example_car.png)

它有三个组件：1个通过摄像头影像来检测出其他的汽车;1个检测出行人；最后1个规划出可以规避其他汽车和行人的行车路径。

"pipeline"中的每一个组件不是都必须涉及到机器学习算法。例如，最后的汽车路径规划步骤中的“机器人运动规划”，相关的文献算法有很多。但这些算法，很多并不涉及到机器学习。

![chapter_48_end_end_more_example_car_2.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_48_end_end_more_example_car_2.png)

即使端到端学习已经取得了广泛的成功，但它并不总是一个最好的方法。比如，端到端的语音识别表现地很出色，但我对汽车自动驾驶的端到端学习持怀疑态度。下面几章我会解释原因。

<div class="mk-toclify" id="markdown-toc-58"></div> 

## 49 端到端学习的优点和缺点

考虑我们先前“pipeline”的语音识别系统的例子：

![chapter_48_end_end_more_example_speech.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_48_end_end_more_example_speech.png)

这个pipeline的很多部分都是“人工设计”的：

- MFCCs是人工设计的音频特征集合。虽然这些特征提供了合理的音频输入概述，但它们也通过丢弃一些信息来简化了输入信号。
- 音素是语言学家的发明。它们不能完美地代表音频声音。音素只是现实声音的一个差劲的近似，这导致使用了音素表示的算法，会限制语音识别系统的潜在效果。

这些人工设计的组件限制了语音识别系统的潜在性能。然后，允许人工设计的组件也有一些优点：

- MFCCs特征对语音的某些不影响到说话内容的属性（比如，说话的音高）有非常好的鲁棒性。因此，它们有助于简化学习算法的问题。
- 音素是语音的一个合理的表示，它们同样能有助于学习算法理解基本的语音单元，因此能提高算法的性能。

拥有更多的人工设计的组件通常能允许语音识别系统用更少的数据进行训练。从MFCCs和音素获得的人工设计的知识，是对我们算法从数据得到的知识的一种补充。当我们没有很多数据的时候，这些知识会很有用。

现在，考虑端到端的学习系统。

![chapter_48_end_end_more_example_speech_2.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_48_end_end_more_example_speech_2.png)

这个系统缺少人工设计的知识。因此，当training集比较小的时候，它可能会比人工设计的pipeline系统表现地比更差。

但是，当training集很大的时候，系统的性能不会受到MFCC或者音素表示的限制。如果这个学习算法是一个足够大的神经网络，并且有足够多的数据做训练，它有潜力做得很好，甚至可能接近最佳错误率。

端到端学习系统在有大量标记了“两端（both ends）”的数据的时候，通常会表现地很好。在这个例子里，我们需要大量的（音频，转译文本）对。当这类数据不够的情况下，转到端到端学习系统需要非常的谨慎。

如果你开发的机器学习算法的training集很小，你的算法大多数的知识只能来自你的人类直观知识，也就是你的“人工设计”的组件。

如果你选择了不使用端到端的学习系统，你得决定你的pipeline包含了哪些步骤，以及它们的组合方式。在接下来的几章中，我们会对这些pipeline的设计提出一些建议。

<br>
----------

> 上周，我们学习了端到端学习系统的优点和缺点。对于自动驾驶，我认为一个非端到端的系统实际上会更好些。但当你决定不用端到端学习系统的时候，你如何把一个机器学习的任务分解成更小的系统组件呢？ <br> 请阅读第50-52章 

<br>

<div class="mk-toclify" id="markdown-toc-59"></div> 

## 50 选择pipeline：数据可用性

当我们构建一个非端到端的pipeline系统的时候，pipeline中什么样的组件是好的设计？pipeline的设计会极大地影响到整个系统的性能。一个重要的因素是你是否可以很轻易地收集到用来训练每个组件的数据。

比如，考虑这个自动驾驶的架构：

![chapter_50_auto_car_architecture.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_50_auto_car_architecture.png)

你可以使用机器学习算法来检测汽车和行人。此外，获取这些数据并不困难：有很多计算机视觉数据集都标记了大量的汽车和行人。你也可以使用众包（比如Amazon TealTurk）来获得更大的数据集。因此，我们可以相对容易地获得这些训练数据，来构建汽车检测器和行人检测器。

相反，考虑一个纯端到端的系统：

![chapter_50_auto_car_architecture_end_2_end.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_50_auto_car_architecture_end_2_end.png)

为了训练这个系统，你需要大量的（图像，转向方向）的数据集对。让人们驾驶汽车并记录他们的转向方向来收集这样的数据是非常费时和昂贵的。你需要一个专门装备的车队，以及大量的驾驶行为来覆盖各种可能的场景。这个困难使得这个端到端系统很难训练。相对于获取这些（图像，转向方向）的数据，我们更容易获得大量标记了汽车或行人的图像的数据集。

更一般地说，如果有大量的数据可以用于训练pineline中的“中间模块”（如汽车检测器或行人检测器），那么你可以考虑使用一个有着多个阶段的pipeline系统。这种pipeline的结构设计是更优的，因为你可以使用所有可用的数据来训练中间模块。

在更多端到端的数据变得可用之前，我相信非端到端的系统方法更适用于自动驾驶：它的架构和目前数据的可用性更为匹配。

<div class="mk-toclify" id="markdown-toc-60"></div> 

## 51 选择pipeline：任务简单性

当选择pineline时，除了数据可用性，你要考虑的第二个因素是：单个组件要解决的任务有多简单？你应该尽量选择容易构建或者容易学习的组件。但是，对于一个组件来说，“容易”学习，意味着什么？

![chapter_51_component_easy_to_learn.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_51_component_easy_to_learn.png)

考虑这些按照难度排序的机器学习任务：

1. 对是否过度曝光的图片进行分类（如上面的例子）
2. 对室内或室外拍摄的图片进行分类。
3. 对是否包含猫的图片进行分类。	
4. 对是否包含拥有黑白毛皮的猫的图片进行分类。
5. 对是否包含暹罗猫（一种特定种类的猫）的图片进行分类。

每个任务是一个二分类图片的问题：输入一张图片，输出0或者1。但是对于神经网络来说，这些任务中的前几个任务看起来“很容易”。你可以只通过更少的训练样本来完成这些更容易的分类任务。

机器学习中，还没有一个很好的正式定义出是什么让机器学习的任务变得容易或者困难*（信息论中有“Kolmogorov Complexity”的概念，它表明，一个学习函数的复杂性是生成该函数的最短计算机程序的长度，然而，这一理论概念在AI几乎没有实际应用。参见：[https://en.wikipedia.org/wiki/Kolmogorov_complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity "https://en.wikipedia.org/wiki/Kolmogorov_complexity")）*。随着深度学习和多层神经网络的兴起，我们有时说，如果一个机器学习任务可以用更少的计算步骤来执行（和浅层神经网络对比），那么它是“容易的”，如果它需要更多的计算步骤（需要一个更深层的神经网络），那么它是“困难的”。但这些都是非正式的定义。

如果你能够把一个复杂的任务，将其分解成更简单的子任务，然后通过在每个子任务的步骤中显性的编码（coding），你相当于对算法赋予了某种先验知识，从而帮助它更有效地完成学习的任务。

![chapter_51_siamese_cat.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_51_siamese_cat.png)

假设你在构建一个siamese猫检测器。这是一个纯端到端的系统的架构：

![chapter_51_siamese_cat_end_to_end.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_51_siamese_cat_end_to_end.png)

相反，你可以选择使用2个步骤的pineline系统：

![chapter_51_siamese_cat_pipeline.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_51_siamese_cat_pipeline.png)

第一步骤（猫检测器）检测出图片中的所有的猫。

![chapter_51_siamese_cat_pipeline_detect_all_cat.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_51_siamese_cat_pipeline_detect_all_cat.png)

然后，第二步骤把步骤一检测出来是猫的每张剪辑的图片，（每次一个）传给cat breed分类器，最后如果检测到的猫有任何一只是siamese猫，就输出1。

![chapter_51_siamese_cat_pipeline_detect_output_1.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_51_siamese_cat_pipeline_detect_output_1.png)

与只使用标签0/1来训练的一个纯端到端的分类器相比，pipeline系统中的两个组件--猫检测器和cat breed分类器，看起来更容易学习，并且需要少的多的数据*（如果你对物体检测算法比较熟悉的话，你会知道，它们不只是用0/1图像标签来学习，而是用作为训练数据的一部分来提供的bounding boxes来训练。对它们的讨论超出了本章的范围。如果你想了解关于这些算法的更多知识，请参见Coursera中的深度学习专题[http://deeplearning.ai](http://deeplearning.ai "http://deeplearning.ai")）*。

作为最后一个例子，让我们重新回到自动驾驶的pipeline系统。

![chapter_51_autonomouse_driving_pipeline.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_51_autonomouse_driving_pipeline.png)

通过这个pipeline，我们可以看出，这个系统算法有3个关键的步骤：（1）检测出其他的汽车，（2）检测出行人，和（3）规划一条行驶的路径。此外，每一个步骤都是一个相对简单的的功能函数-因此可以用比纯端到端系统更少的数据来学习。

总的来说，当决定pipeline系统中的组件设计的时候，应该尝试去构建这样的一个pipeline系统，系统中每个组件都是一个相对“简单”的功能函数，因此每个组件都可以只使用少量的数据来学习和训练。

<div class="mk-toclify" id="markdown-toc-61"></div> 

## 52 直接学习富输出（rich outputs）

一个图片分类器算法输入一张图片x，输出一个表示一个对象类别的整数。算法能输出一个完整的描述这张图片的句子吗？

比如：

![chapter_52_image_to_sentence.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_52_image_to_sentence.png)

传统的有监督学习算法学习的函数h：X->Y，输出y通常是一个整数或者一个实数。比如：

| 问题          	|        X          	|       Y             	|
| ------------- |:-------------------:	|:------------------:	|
| 垃圾邮件分类  	|      邮件         		|     垃圾/非垃圾（0/1) 	|
| 图像分类   	|      图像         		|     整数标签         	|
| 房价预测    	|      房子的特征    	|    美元价格          	|
| 产品推荐	   	|      产品&用户特征  	|	  购买的几率			|

端到端深度学习中，其中一个最让人兴奋的发展是，我们可以直接学习比一个数字更为复杂的对象y。在上面的图像-标题识别的例子中，你可以让一个神经网络输入一张图片（x），然后直接输出一个标题（y）。

这里有更多的例子：

| 问题      		 	|        X          	|       Y         	|   引用论文实例				|
| :---------------:	|:-------------------:	|:-------------:	|:-----------------: 		|
| 图像生成标题    	|      图像         		|     文本		   	|	Mao et al.,2014 			|
| 机器翻译        	|      英语文本       	|    法语文本    	| Suskever et al., 2014		|
| 问题自动回答    	|      （文本，问题）对	|   答案文本    		| Bordes et al., 2015		|
| 语音识别        	|      音频      		|    转译文本   		| Hannun et al., 2015		|
| 语音引擎TTS     	|      文本特征      	|    音频		    | van der Oord et al., 2016	|

这是深度学习的一种越来越快的趋势：当你有正确标注的（输入，输出）对，即便输出是一句话，一张图片，音频，或者是其他比单个数字更丰富的形式，你有时候也可以使用端到端的深度学习系统。

<br>

----------

> 下面是machine learning yearning的最后几章。你会学习到机器学习pipeline上的误差分析：如果你有一个复杂的系统，如自动驾驶系统，这个系统由很多子组件组成，那么你怎么决定先去处理哪个子组件？我希望这几篇文章能够帮助你和你的团队更加有效地推进机器学习项目。

<br>

<div class="mk-toclify" id="markdown-toc-62"></div> 

# 对系统的每个组件做错误分析

<br>

<div class="mk-toclify" id="markdown-toc-63"></div> 

## 53 对pipeline的每个组件的错误分析

假设你的系统是一个有着复杂的pipeline的机器学习系统，你希望提高这个系统的性能。你应该去优化pipeline中的哪个部分？通过把错误归因到某些pipeline的子系统，你可以更好地安排你的工作。

让我们用之前的Siamese猫分类器举例子：

![chapter_53_cat_classifier_pipeline.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_53_cat_classifier_pipeline.png)

第一部分，是一个猫检测器，从图片中检测并裁剪出猫。第二部分是Siamese猫分类器，检测是否是Siamese猫。改进这两个部分中的任何一个，都可能需要花费几年的时间。你怎么决定选择去改进哪个部分？

通过对每个部分组件进行**误差分析**,你可以尝试把算法导致的每个错误都归因于pipeline的某个部分（有时候是两个部分）。举个例子，算法错误地把下面这张图片判断为没有包含Siamese猫，虽然这张图片的正确的label是1.

![chapter_53_cat_mis_classifier.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_53_cat_mis_classifier.png)

接下来，我们手动地检查一下，这两个部分的算法分别做了什么？假设猫检测器对这张图片的识别结果如下：

![chapter_53_cat_mis_classifier_Siamese.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_53_cat_mis_classifier_Siamese.png)

这表示，猫检测器认为下图是一只猫。

![chapter_53_cat_mis_classifier_Siamese_part.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_53_cat_mis_classifier_Siamese_part.png)

接着，Siamese猫分类器正确地识别出上图没有包含一只Siamese猫。因此Siamese猫分类器是没错的。它输入一堆石头，输出一个很合理的y=0的label。事实上，人们预测上述的裁剪图片也会得出y=0的结果。因此，你可以很明确地把这个错误归因于是猫检测器的问题。

如果，从另一方面说，猫检测器输出的是下图：

![chapter_53_cat_mis_classifier_Siamese_correct.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_53_cat_mis_classifier_Siamese_correct.png)

那么你可以很明确地确定猫检测器没问题，有问题的是Siamese猫分类器。

假如你分析了100个错误case，发现有90个是归因于猫检测器，10个是归因于Siamese猫分类器。那么你应该很明确地优先去优化猫检测器。

而且，你已经找到了90个猫检测器的输出错误bounding图片的case了，那么你可以对90个case来进行更深层次的误差分析，用来提升你的猫检测器。

一个非正式的关于如何把错误case归因于pipeline的某个部分的方法如下所示：观察每一部分的输出，看看能否决定哪一个部分的输出错了。这种非正式的方法可能是你所需要的。但在下一章，你也会看到一种更为正式的错误归因的方式。

<div class="mk-toclify" id="markdown-toc-64"></div> 

## 54 把错误归因到pipeline的每个组件

让我们继续使用之前的例子：

![chapter_53_cat_classifier_pipeline.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_53_cat_classifier_pipeline.png)

假设猫检测器输出的是下图的bound：

![chapter_54_error_attrbuite_wrong_cropped.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_54_error_attrbuite_wrong_cropped.png)

Siamese猫分类器输入的是如下的裁剪图片，然后不正确地输出y=0（表示这图片没有包含猫）。

![chapter_54_error_attrbuite_wrong_cropped_wrong_classifier.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_54_error_attrbuite_wrong_cropped_wrong_classifier.png)

猫检测器犯了个大错，然而，一个有经验的人类仍然可以从错误裁剪的图片中识别出Siamese猫。所以，我们应该把这个错误归因于猫检测器还是Siamese猫分类器呢，或者是两者？这是有争议的。

如果这些有争议的case数目比较小，你可以把这个错误归因到任意一方，得到的结果都差不多。但是，这里有一个更正式的测试，让你更明确地把错误归因于某一方：

1. 用人工标注的bounding box来替换猫检测器的输出
![chapter_54_error_attrbuite_wrong_cropped_2.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_54_error_attrbuite_wrong_cropped_2.png)
2. 通过Siamese猫分类器跑一下对应的被裁剪的图片。如果Siamese猫分类器分类错了，就把这个错误归因于Siamese猫分类器。否则，把这个错误归因于猫检测器。

换句话说，跑一个实验，把Siamese猫分类器的输入变成"完美"的输入，这样会有两种情况：

1. 情况1：即使输入了完美的输入，Siamese猫分类器仍然错误地输出y=0。在这种情况下，很明显，是Siamese猫分类器的错误。
2. 情况2：输入了完美的输入，Siamese猫分类器正确地输出了y=1。这表示，如果猫检测器输出了一个更准确的输出，那么整个系统的输出就会变正确。那么，把这个错误归因于猫检测器。

通过对dev集中错误分类的图片进行这种分析，你可以毫无争议地把每个错误归因于系统的每个部分。这种方法能够允许你预估出系统每部分的错误比例，从而可以据此来决定你的优化方向.

<div class="mk-toclify" id="markdown-toc-65"></div> 

## 55 错误归因的一般情形

如下是错误归因的一般步骤。假设某系统的pipeline有三个步骤A，B和C，A的输出结果是B的输入，B的输出结果是C的输入。

![chapter_54_error_attrbuite_general_steps.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_54_error_attrbuite_general_steps.png)

对于该系统在dev集上的每个错误：

1. 人工地把A的输出变成“完美的”输出（比如，正确的猫框框图片），然后用这个输出，运行剩余的B和C。如果系统给出的是正确的结果，那么我们可以说，如果A能够输出更准确的结果，那么整体系统的输出就会变得准确。因此，你可以把这个错误归因于A。否则，跳转到步骤2。
2. 人工地把B的输出变成“完美的”输出。如果系统这个时候输出正确的结果，那么，把这个错误归因于B，否则，跳转到步骤3。
3. 把这个错误归因到C。

让我们看一个更复杂的例子：

![chapter_54_error_attrbuite_general_steps_more_complex.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_54_error_attrbuite_general_steps_more_complex.png)

假设你的自动驾驶系统使用的是如上类似的pipeline，那么你怎么使用错误分析的方法来决定应该优化哪个组件？

你可以把这三个组件映射成A、B和C，如下所示：

A： 汽车检测组件
B： 行人检测组件
C： 汽车行车规划组件

根据上述的过程，假设你在封闭的道路上测试你的汽车，并找到这样的case，相比比熟练的驾驶员，你的汽车选择了一个更颠簸的方向。在自驾车的世界中，这种case通常被称为**情景（scenario）**。然后，你会如下做：

1. 人工地把A（汽车检测组件）的输出变成“完美的”输出（比如，手动告诉其他的车在哪里），然后用这个输出，运行剩余的B和C，不过让C（行车规划组件）使用的是A的“完美”输出。如果系统能够规划出一条更好的路径，那么我们可以说，如果A能够输出更准确的结果，那么整体系统的输出就会变得准确。因此，你可以把这个错误归因于A。否则，跳转到步骤2。
2. 人工地把B（行人检测组件）的输出变成“完美的”输出。如果系统这个时候输出正确的结果，那么，把这个错误归因于B，否则，跳转到步骤3。
3. 把这个错误归因到C。

ML pipeline的组件可以根据有向无环图（DAG）进行排序，这意味着您应该能够以某种固定的从左到右的顺序计算它们，而后面的组件应该只依赖于之前组件的输出。只要组件关系A-＞B-> C顺序遵循DAG排序，那么上述的错误分析的方法就是正确的。如果交换A和B，可能会得到稍微不同的结果。

A： 行人检测组件（之前是汽车检测组件）
B： 汽车检测组件（之前是行人检测组件）
C： 行车路线规划组件

但是，这种分析的结果仍然是有效的，并为你的优化方向提供了很好的指导。

<div class="mk-toclify" id="markdown-toc-66"></div> 

## 56 对各组件的错误分析和与人的水平表现比较

对一个学习算法进行错误分析和使用数据科学来分析机器学习系统的错误类似，都是为了得出下一步该做什么。在最基本的情况，对各组件的错误分析，能够告诉我们哪个组件的性能最值得花费最大的力气来提升。

假设你有一个某个网站的用户购买记录的数据集。一个数据科学家可能有很多方法来分析这些数据。她可能得到很多结论，比如该网站是否需要提升价格，如何通过不同的营销活动获得的客户的终身价值。分析数据集并没有一个“正确”的方法，并且我们能得到很多可能有用的推论见解。同样地，错误分析也没有一个“正确”的方法。通过这些章节，你可以学习到很多最常见的设计模式，来对你的机器学习系统进行剖析，得出有效的结论，但是你应该也要自由地尝试其他分析错误的方法。

让我们回到自动驾驶的例子，一个汽车检测算法输出附近的车辆的位置（可能还有速度），一个行人检测算法输出附近的行人，这两者的输出最后用来做汽车的行车路线的规划。

![chapter_54_error_attrbuite_general_steps_more_complex.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_54_error_attrbuite_general_steps_more_complex.png)

为了去debug这个pipeline，而不是严格地按照之前章节描述的步骤，你可以更非正式地提如下的问题：

1. 汽车检测组件离人类水平表现相比，还差多远？
2. 行人检测组件离人类水平表现相比，还差多远？
3. 整个系统离人类水平表现相比，还差多远？ 这里的人类水平表现假设人类只能在看到这两个检测组件的输入的情况下去规划驾驶路线（而不是看到摄像头图片）。换句话说，在同样的输入情况下，行车路线规划组件的性能，离人类水平表现相比，还差多远？

如果你发现某一个组件的性能，离人类水平表现差得很远，那么，现在，您有一个很好的理由来关注如何提高该组件的性能。

当我们试图让人类做的事情变成自动化的时候，错误分析的方法最有效，因此我们可以用人类水平表现进行基准测试。我们前面的大多数例子都隐含了这种假设。如果你正在构建一个ML系统，系统的最终输出或者某些中间组件执行的事情，即使是人类也无法做得很好，那么这些方法的某一些将不再适用。

这也是解决人类可以解决的问题的另一个优点--你有更强大的的错误分析的工具，然后你可以更有效地对你团队工作进行规划。

<div class="mk-toclify" id="markdown-toc-67"></div> 

## 57 发现有缺陷的ML pipeline

如果每个组件的性能已经和人类水平表现齐平或者相差不多，但整个系统的性能却远不如人类水平表现，那该怎么办？这通常表示，你的系统的pipeline是有缺陷的，需要重新设计。错误分析也同样能帮你确定是否需要重新设计你的pipeline。

![chapter_54_error_attrbuite_general_steps_more_complex.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_54_error_attrbuite_general_steps_more_complex.png)

在之前的章节，我们提出了三个问题，这三个组件的性能是否都和人类水平表现齐平。假设这三个问题的答案都是“是的”。

1. 汽车检测组件从摄像头图片中识别汽车的能力和人类水平表现齐平（大概地）。
2. 行人检测组件从摄像头图片中识别行人的能力和人类水平表现齐平（大概地）。
3. 在只输入前两个组件的输出的情况下做行车路线规划（而不是输入摄像头图片），行车路线规划组件和人类水平表现在一个水平上。

然而，你的整个系统的性能却远比人的水平表现要差。比如，看到摄像头图片的人类能够规划出好得多的行车路线。你能得到怎样的结论？

唯一有可能的结论是这个ML pipline是有缺陷的。在这种例子中，行车路线规划组件在**它的输入**的情况下表现得很好，但这些输入并没有包含足够的信息。你应该问问自己，为了汽车能够行驶地很好，除了之前那两个pipeline组件的输入外，还有什么其他的信息是必要的。换句话说，	对于一个有经验的人类司机来说，什么其他的信息是必需的?

比如说，假设你注意到，一个人类司机需要知道车道线的位置。那表明你应该像下图一样设计你的pipeline（*在上述的自动驾驶例子，理论上我们可以把原始的摄像头图片输入到行车路线规划组件，来解决这个问题。但是，这个会违反第51章描述的“任务简单化”的设计原则，因为行车路线规划组件需要输入原始的摄像图图片，并需要解决一个很复杂的任务。这就是为什么增加一个车道线检测组件是一个更好的选择--它可以帮助行车路线规划组件获得之前缺失但很重要的车道线信息，并且可以避免要构建一个很复杂的模块。*）：

![chapter_57_Spotting_a_flawed_ML_pipeline.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_57_Spotting_a_flawed_ML_pipeline.png)

总而言之，如果你的系统中每一个组件已经具有了人的水平表现的性能了，但整个系统的性能还是比不上人的水平表现(记住，对比的人类应该和组件一样有同样的输入），那么你的pipeline是有缺陷的，应该重新设计。


<div class="mk-toclify" id="markdown-toc-68"></div> 

# 结论

<div class="mk-toclify" id="markdown-toc-69"></div> 

## 58 打造一个超级英雄团队 - 让你的队友阅读本书

祝贺你阅读完这本书！

在章节2，我们提到，这本书能够帮忙你打造你的超级英雄团队。

![chapter_58_Building_a_superhero_team.png](https://raw.githubusercontent.com/bbskill/translate/master/books/Machine_Learning_Yearning/images/chapter_58_Building_a_superhero_team.png)

唯一比成为一名超级英雄更好的是成为超级英雄团队的一员。我希望你能把这本书推荐给你的朋友和队友，并帮助他们打造其他更多的超级英雄！
